{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- ü§ù Breakout Room #2:\n",
        "  - Part 1: LangSmith Evaluator:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "  - Part 2:\n",
        "    3. Adding Helpfulness Check and \"Loop\" Limits\n",
        "    4. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "3b97db0d-d119-4b43-b964-47291c7dba1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "761167a9-b570-421b-eb9c-8be3dc813f47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b0237b19-ada7-4836-edc2-228e694a5ecb"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE3 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/fab950acfbf5fea46c9313dca34ee2ae01f1728b/libs/langgraph/langgraph/prebuilt/tool_executor.py#L50) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "Based on the prompt, tool descriptions and the input.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "There is not limit to how many we can cycle.\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "We can add cycle counter to the execution graph and then define maximum number of cycles and send graph to termination node once limit is reached.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "90f7d3dc-0fe2-4d1f-8221-9b3bcffc982e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Agent Response: RAG, or Retrieval-Augmented Generation, is a technique used in the context of large language models (LLMs) to improve their performance by combining retrieval-based methods with generative models. The core idea is to enhance the generative capabilities of LLMs by incorporating relevant information retrieved from a large corpus of documents or a knowledge base. This approach helps the model generate more accurate and contextually relevant responses, especially for tasks that require specific knowledge or up-to-date information.\n",
            "\n",
            "### Key Components of RAG:\n",
            "1. **Retriever**: This component searches a large corpus to find relevant documents or passages based on the input query.\n",
            "2. **Generator**: This component generates a response using both the input query and the retrieved documents. Typically, a pre-trained language model like GPT-3 or BERT is used for this purpose.\n",
            "\n",
            "### How RAG Works:\n",
            "1. **Query Input**: The user provides a query or prompt.\n",
            "2. **Document Retrieval**: The retriever searches a large corpus to find documents that are relevant to the query.\n",
            "3. **Response Generation**: The generator uses the retrieved documents along with the original query to generate a coherent and contextually accurate response.\n",
            "\n",
            "### Breakthrough and Adoption:\n",
            "RAG was introduced by Facebook AI (now Meta AI) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The technique quickly gained attention for its ability to improve the performance of language models on tasks that require access to external knowledge, such as open-domain question answering and fact-checking.\n",
            "\n",
            "The introduction of RAG marked a significant advancement in the field of natural language processing (NLP) by demonstrating how combining retrieval and generation can lead to more powerful and versatile language models.\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "6ae3afec-7da7-4c5d-a3ca-2ed75735c029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2024-05-27\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technologies derived from the perspective of unified information: (1)\n",
            "statistics-based Information Calibration Quantization allows the quantized\n",
            "parameters of LLM to retain original information accurately; (2)\n",
            "finetuning-based Information Elastic Connection makes LoRA utilizes elastic\n",
            "representation transformation with diverse information. Comprehensive\n",
            "experiments show that IR-QLoRA can significantly improve accuracy across LLaMA\n",
            "and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\n",
            "improvement on MMLU compared with the state-of-the-art methods. The significant\n",
            "performance gain requires only a tiny 0.31% additional time consumption,\n",
            "revealing the satisfactory efficiency of our IR-QLoRA. We highlight that\n",
            "IR-QLoRA enjoys excellent versatility, compatible with various frameworks\n",
            "(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\n",
            "The code is available at https://github.com/htqin/ir-qlora.\n",
            "\n",
            "Published: 2024-06-12\n",
            "Title: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\n",
            "Authors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
            "Summary: There are various methods for adapting LLMs to different domains. The most\n",
            "common methods are prompting, finetuning, and RAG. In this w\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers bio\"}\n",
            "Tool Response: Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers is a graduating PhD student advised by Luke Zettlemoyer at the University of Washington in Seattle. He holds degrees in applied math and computer science and has a background in industrial automation. His primary research goal is to democratize foundation models by making them more efficient and accessible through quantization ...\n",
            "\n",
            "Agent Response: ### What is QLoRA in Machine Learning?\n",
            "\n",
            "QLoRA (Quantized Low Rank Adapters) is an efficient finetuning approach designed to reduce memory usage while preserving the performance of large language models (LLMs). The key features of QLoRA include:\n",
            "\n",
            "1. **Memory Efficiency**: It allows finetuning of a 65 billion parameter model on a single 48GB GPU.\n",
            "2. **Quantization**: Utilizes 4-bit quantization of pretrained language models.\n",
            "3. **Low Rank Adapters (LoRA)**: Backpropagates gradients through these adapters.\n",
            "4. **Innovations**:\n",
            "   - **4-bit NormalFloat (NF4)**: A new data type optimal for normally distributed weights.\n",
            "   - **Double Quantization**: Reduces memory footprint by quantizing the quantization constants.\n",
            "   - **Paged Optimizers**: Manages memory spikes.\n",
            "\n",
            "The approach has been shown to achieve state-of-the-art results with smaller models and less computational resources. The paper also discusses the limitations of current chatbot benchmarks and provides a detailed analysis of chatbot performance.\n",
            "\n",
            "### Technical Papers on QLoRA\n",
            "\n",
            "1. **Title**: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "   - **Summary**: This paper introduces QLoRA and demonstrates its effectiveness in finetuning large language models with reduced memory usage while maintaining performance.\n",
            "\n",
            "2. **Title**: [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "   - **Summary**: This paper proposes IR-QLoRA, which enhances the accuracy of quantized LLMs through information retention techniques.\n",
            "\n",
            "3. **Title**: [Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\n",
            "   - **Summary**: This paper explores the adaptation of LLMs to different domains using QLoRA and assesses the quality of the models.\n",
            "\n",
            "### Bio of Tim Dettmers\n",
            "\n",
            "Tim Dettmers is a PhD student at the University of Washington in Seattle, advised by Luke Zettlemoyer. He holds degrees in applied mathematics and computer science and has a background in industrial automation. His research focuses on making foundation models, such as ChatGPT, more accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning. His primary research goal is to democratize foundation models by making them more efficient and accessible through quantization and other techniques.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "1. get input question\n",
        "2. decided to call tool arxiv with Query: {\"query\":\"QLoRA\"} for the part of question related to the Qlora\n",
        "3. ten it called duckduckgo_search with Query: {\"query\":\"Tim Dettmers bio\"} for that part of question\n",
        "4. combines to outputes and generate response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "d2b3be9d-1dbb-4109-83e4-673c9b73483e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here‚Äôs a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus based on the input query. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input query. This can involve concatenating the retrieved information with the original query or using it to provide additional context.\\n\\n3. **Generation**: Finally, a generative model, such as a transformer-based language model (e.g., GPT-3, BERT), uses the augmented input to generate a response. The generative model can produce more accurate and contextually relevant answers by leveraging the additional information provided by the retrieval step.\\n\\nRAG models are particularly useful in scenarios where the information needed to answer a query is not contained within the model's parameters but can be found in external documents. This approach helps in generating more accurate and informative responses, especially for complex queries that require specific knowledge.\""
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What quantization techniques are used in QLoRA?\",\n",
        "    \"How does QLoRA improve model efficiency?\",\n",
        "    \"How does LlamaIndex optimize information retrieval?\",\n",
        "    \"What are the key features of LlamaIndex?\",\n",
        "    \"What is the role of Hidden Markov Models in NLP?\",\n",
        "    \"What are the differences between ReAct and Worker agents?\",\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"quantization\", \"techniques\"]},\n",
        "    {\"must_mention\" : [\"efficiency\", \"performance\"]},\n",
        "    {\"must_mention\" : [\"optimize\", \"information\"]},\n",
        "    {\"must_mention\" : [\"features\", \"capabilities\"]},\n",
        "    {\"must_mention\" : [\"sequence\", \"Kurzweil\"]},\n",
        "    {\"must_mention\" : [\"prompt\", \"approaches\"]},\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset(name='Retrieval Augmented Generation - Evaluation Dataset - d9202f24', description='Questions about the QLoRA Paper to Evaluate RAG over the same paper.', data_type=<DataType.kv: 'kv'>, id=UUID('863005e9-ef2f-455c-af4e-14f56c31567e'), created_at=datetime.datetime(2024, 7, 20, 2, 5, 18, 91080, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2024, 7, 20, 2, 5, 18, 91080, tzinfo=datetime.timezone.utc), example_count=0, session_count=0, last_session_start_time=None)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not\n",
        "\n",
        "So the order of the dataset is different on langsmith from how we defined our question localy and metadata could improve this system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "This does not consider case sesnstive approach, spelling mistakes and semantic context, we could use fuzzy matching and other string similarity algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "157f1019-4ee4-4994-fe80-7afc2e5fdfb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 5689b576' at:\n",
            "https://smith.langchain.com/o/fa8256c8-710f-5d46-b883-cf53bff74698/datasets/863005e9-ef2f-455c-af4e-14f56c31567e/compare?selectedSessions=0ed68875-b163-4ad7-beaa-ed24c1cf47ef\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - d9202f24 at:\n",
            "https://smith.langchain.com/o/fa8256c8-710f-5d46-b883-cf53bff74698/datasets/863005e9-ef2f-455c-af4e-14f56c31567e\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.COT Contextual Accuracy</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5658984a-b78b-4f84-86ea-5fb3e32dccdb</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.452208</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.447214</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.258824</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.294933</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.633388</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.216861</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.638388</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.766800</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.COT Contextual Accuracy  \\\n",
              "count                    6.0                          5.000000   \n",
              "unique                   NaN                               NaN   \n",
              "top                      NaN                               NaN   \n",
              "freq                     NaN                               NaN   \n",
              "mean                     1.0                          0.800000   \n",
              "std                      0.0                          0.447214   \n",
              "min                      1.0                          0.000000   \n",
              "25%                      1.0                          1.000000   \n",
              "50%                      1.0                          1.000000   \n",
              "75%                      1.0                          1.000000   \n",
              "max                      1.0                          1.000000   \n",
              "\n",
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      6     0        6.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                     True   NaN             NaN   \n",
              "freq                       4   NaN             NaN   \n",
              "mean                     NaN   NaN        5.452208   \n",
              "std                      NaN   NaN        1.258824   \n",
              "min                      NaN   NaN        4.294933   \n",
              "25%                      NaN   NaN        4.633388   \n",
              "50%                      NaN   NaN        5.216861   \n",
              "75%                      NaN   NaN        5.638388   \n",
              "max                      NaN   NaN        7.766800   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     5658984a-b78b-4f84-86ea-5fb3e32dccdb  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 5689b576',\n",
              " 'results': {'bd9dcdb4-0f04-4fda-a519-16e296ae3a9c': {'input': {'question': 'What quantization techniques are used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of the quantization techniques used in QLoRA. It lists and explains five different techniques, providing insight into how each one works and why it is used. The information is presented in a clear and organized manner, making it easy to understand. \\n\\nThe submission is also appropriate for the given input. It directly answers the question asked and provides additional context to help the reader understand the topic better. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('e1a3dbf0-b7d5-414f-8de5-55ca6dc367e8'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=\"The student's answer provides a detailed explanation of the quantization techniques used in QLoRA. The student mentions 8-bit Quantization, 4-bit Quantization, Mixed-Precision Quantization, Dynamic Quantization, and Quantization-Aware Training (QAT). However, the context provided does not give any specific techniques used in QLoRA. Therefore, we cannot verify the accuracy of the student's answer based on the given context. \\nGRADE: INCORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('8b9fedc0-769c-4573-a840-c74869c2b235'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('92eceebc-488d-4d37-90ba-6068222d00eb'), target_run_id=None)],\n",
              "   'execution_time': 4.294933,\n",
              "   'run_id': '5658984a-b78b-4f84-86ea-5fb3e32dccdb',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) is a technique used to fine-tune large language models efficiently by leveraging quantization and low-rank adaptation. The quantization techniques used in QLoRA typically include:\\n\\n1. **8-bit Quantization**: This involves reducing the precision of the model weights from 16-bit or 32-bit floating-point numbers to 8-bit integers. This significantly reduces the memory footprint and computational requirements while maintaining a reasonable level of accuracy.\\n\\n2. **4-bit Quantization**: In some cases, even more aggressive quantization to 4-bit integers is used. This further reduces the memory and computational load but requires more sophisticated techniques to maintain model performance.\\n\\n3. **Mixed-Precision Quantization**: This technique involves using different levels of precision for different parts of the model. For example, some layers or operations might use 8-bit precision while others use 16-bit or 32-bit precision. This allows for a balance between efficiency and accuracy.\\n\\n4. **Dynamic Quantization**: This involves quantizing the model weights dynamically during inference or training, rather than statically quantizing them beforehand. This can help in adapting to different input data distributions and maintaining performance.\\n\\n5. **Quantization-Aware Training (QAT)**: This technique involves simulating the effects of quantization during the training process. By doing so, the model can learn to be more robust to the quantization process, leading to better performance when the model is actually quantized.\\n\\nThese quantization techniques are combined with low-rank adaptation methods to fine-tune large models efficiently, making QLoRA a powerful approach for deploying large language models in resource-constrained environments.',\n",
              "   'reference': {'must_mention': ['quantization', 'techniques']}},\n",
              "  '92326cac-52db-4fa1-a5c2-9b9f2816e823': {'input': {'question': 'How does QLoRA improve model efficiency?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". To determine if the submission meets this criterion, I will consider the following:\\n\\n1. **Helpfulness**: The submission provides a detailed explanation of how QLoRA improves model efficiency. It breaks down the concept into four key areas: Quantization, Low-Rank Adaptation, Memory Efficiency, and Maintaining Performance. Each area is further explained with specific points, making the information easy to understand. The submission also provides a summary at the end, reinforcing the main points. This makes the submission very helpful for someone trying to understand how QLoRA works.\\n\\n2. **Insightfulness**: The submission goes beyond a basic explanation of QLoRA. It provides insights into how the technique works, the benefits it provides, and how it can be used to optimize large language models. This makes the submission insightful.\\n\\n3. **Appropriateness**: The submission directly answers the input question and stays on topic throughout. It uses appropriate language and terminology, making it suitable for the intended audience.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('76d19f7b-58c2-45ed-b232-5b8cca5d4c22'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is comprehensive and detailed, explaining how QLoRA improves model efficiency. The student correctly identifies the two main techniques used by QLoRA: quantization and low-rank adaptation. They also correctly explain how these techniques reduce computational and memory requirements, and how they maintain the performance of the model. The student's answer is consistent with the context provided, and does not contain any conflicting statements. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('20cc4289-d275-4f25-95d2-23f10b0ad4d4'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('c12538e7-26b0-4619-b6b3-bfbc016f8ef4'), target_run_id=None)],\n",
              "   'execution_time': 5.651879,\n",
              "   'run_id': '58438505-6911-4402-8b69-df87bc687e2c',\n",
              "   'output': \"QLoRA (Quantized Low-Rank Adaptation) is a technique designed to improve the efficiency of large language models (LLMs) by reducing their computational and memory requirements. Here are the key ways in which QLoRA achieves this:\\n\\n1. **Quantization**:\\n   - **Reduced Precision**: QLoRA uses quantization to reduce the precision of the model's weights and activations. Instead of using 32-bit floating-point numbers, it uses lower precision formats like 8-bit integers. This significantly reduces the memory footprint and computational requirements.\\n   - **Efficient Storage**: Quantized models require less storage space, making it easier to deploy them on devices with limited memory, such as mobile phones or edge devices.\\n\\n2. **Low-Rank Adaptation**:\\n   - **Parameter Efficiency**: QLoRA applies low-rank adaptation techniques to the model's weight matrices. By approximating these matrices with lower-rank representations, the number of parameters is reduced without significantly compromising the model's performance.\\n   - **Faster Training and Inference**: With fewer parameters to update and compute, both the training and inference processes become faster. This is particularly beneficial for large models where computational resources are a bottleneck.\\n\\n3. **Memory Efficiency**:\\n   - **Reduced Memory Bandwidth**: Lower precision and fewer parameters mean that less data needs to be transferred between memory and processing units. This reduces the memory bandwidth requirements, which is often a limiting factor in high-performance computing.\\n   - **Scalability**: The reduced memory footprint allows for larger models to be trained and deployed on the same hardware, or for the same model to be run on less powerful hardware.\\n\\n4. **Maintaining Performance**:\\n   - **Minimal Performance Loss**: Despite the reduction in precision and parameters, QLoRA is designed to maintain the model's performance as much as possible. The low-rank adaptation ensures that the most important features of the weight matrices are preserved.\\n   - **Fine-Tuning**: QLoRA can be used in conjunction with fine-tuning techniques to further optimize the model's performance on specific tasks, ensuring that the efficiency gains do not come at the cost of accuracy.\\n\\nIn summary, QLoRA improves model efficiency by combining quantization and low-rank adaptation to reduce the computational and memory requirements of large language models, while maintaining their performance. This makes it possible to deploy powerful models on resource-constrained devices and to speed up both training and inference processes.\",\n",
              "   'reference': {'must_mention': ['efficiency', 'performance']}},\n",
              "  'a379de4c-960a-4f85-b403-603010c6b11c': {'input': {'question': 'How does LlamaIndex optimize information retrieval?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". To determine if the submission meets this criterion, we need to assess if the information provided is helpful, insightful, and appropriate.\\n\\n1. **Helpfulness**: The submission provides a detailed explanation of how LlamaIndex optimizes information retrieval. It breaks down the process into seven key strategies, explaining each one in a way that is easy to understand. This makes the information helpful to someone trying to understand the concept.\\n\\n2. **Insightfulness**: The submission goes beyond a basic explanation of LlamaIndex. It provides insights into how the system works, including its integration with large language models, its scalability, and its support for real-time updates and multi-modal data. This depth of information makes the submission insightful.\\n\\n3. **Appropriateness**: The submission directly answers the question asked in the input. It stays on topic and provides relevant information about LlamaIndex and its optimization of information retrieval. This makes the submission appropriate.\\n\\nBased on this analysis, the submission meets the criterion of helpfulness. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('f9460745-79b1-48a0-a5cf-f51d2ddacc80'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer provides a detailed explanation of how LlamaIndex optimizes information retrieval. The student mentions several key strategies such as data indexing, query optimization, integration with large language models, scalability, customization, real-time updates, and multi-modal data support. All these points are relevant to the question and provide a comprehensive understanding of how LlamaIndex works. The student's answer does not contradict the context given. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2a84862d-9911-4096-885e-c47feb307208'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('b0c31488-8005-42d8-b692-c01c02d5125f'), target_run_id=None)],\n",
              "   'execution_time': 4.565915,\n",
              "   'run_id': '41a51e35-2fe6-4def-9819-2e89771a3304',\n",
              "   'output': 'LlamaIndex, also known as GPT Index, is a data framework designed to facilitate the connection between large language models (LLMs) and external data sources. It optimizes information retrieval through several key strategies:\\n\\n1. **Data Indexing**: LlamaIndex creates structured indices from various data sources, such as documents, databases, and APIs. These indices are designed to be efficiently searchable, allowing for quick retrieval of relevant information.\\n\\n2. **Query Optimization**: The framework uses advanced query optimization techniques to ensure that searches are both fast and accurate. This involves the use of algorithms that can quickly narrow down the search space to the most relevant documents or data points.\\n\\n3. **Integration with LLMs**: By integrating with large language models, LlamaIndex leverages the powerful natural language understanding capabilities of these models to interpret and respond to queries more effectively. This integration allows for more nuanced and context-aware information retrieval.\\n\\n4. **Scalability**: LlamaIndex is built to handle large-scale data, making it suitable for enterprise-level applications. It can manage and search through vast amounts of information without significant performance degradation.\\n\\n5. **Customization**: Users can customize the indexing and retrieval process to better suit their specific needs. This includes the ability to define custom data schemas, relevance scoring mechanisms, and more.\\n\\n6. **Real-time Updates**: The framework supports real-time updates to the indices, ensuring that the information retrieved is always up-to-date. This is particularly useful for applications that require the latest data, such as news aggregation or financial analysis.\\n\\n7. **Multi-modal Data Support**: LlamaIndex can handle various types of data, including text, images, and structured data. This multi-modal capability allows for more comprehensive information retrieval across different data formats.\\n\\nBy combining these strategies, LlamaIndex provides a robust and efficient solution for information retrieval, making it easier to access and utilize large volumes of data.',\n",
              "   'reference': {'must_mention': ['optimize', 'information']}},\n",
              "  '15a89f23-b186-40a6-8cff-effac2269578': {'input': {'question': 'What are the key features of LlamaIndex?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it provides a detailed explanation of the key features of LlamaIndex. It covers various aspects such as data integration, indexing, query processing, scalability, customization, performance optimization, security, interoperability, user-friendly interface, and documentation and support. \\n\\nThe submission is helpful as it provides a comprehensive understanding of what LlamaIndex is and what it offers. It is insightful as it goes beyond just listing the features and explains what each feature means and why it is important. \\n\\nThe submission is also appropriate as it directly answers the question asked in the input. It stays on topic and provides relevant information.\\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4db4c163-5b05-412f-a098-8e4bbc7449da'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=None, value=None, comment=\"The student's answer provides a comprehensive list of features for LlamaIndex, also known as GPT Index. The student mentions data integration, indexing, query processing, scalability, customization, performance optimization, security, interoperability, user-friendly interface, and documentation and support as key features. However, the context provided does not give any specific features of LlamaIndex to compare the student's answer to. Therefore, without additional context, it is impossible to determine the factual accuracy of the student's answer. \\nGRADE: CANNOT BE DETERMINED\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2dc449c8-d482-4964-b3a8-311fdfca5d01'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('87f17b58-0c8b-41ea-a777-4e3fd95816e6'), target_run_id=None)],\n",
              "   'execution_time': 4.835805,\n",
              "   'run_id': 'ce367f5a-f07e-4e10-a865-016585b94e77',\n",
              "   'output': 'LlamaIndex, also known as GPT Index, is a data framework designed to facilitate the connection of Large Language Models (LLMs) with external data sources. Here are some of its key features:\\n\\n1. **Data Integration**: LlamaIndex allows for seamless integration with various data sources, including documents, databases, and APIs. This enables LLMs to access and utilize external data effectively.\\n\\n2. **Indexing**: It provides robust indexing capabilities, allowing for efficient organization and retrieval of information from large datasets. This is crucial for enabling LLMs to quickly find relevant information.\\n\\n3. **Query Processing**: LlamaIndex supports advanced query processing, enabling users to perform complex searches and retrieve precise information from indexed data.\\n\\n4. **Scalability**: The framework is designed to handle large-scale data, making it suitable for enterprise-level applications where vast amounts of information need to be managed and accessed.\\n\\n5. **Customization**: Users can customize the indexing and query processing mechanisms to suit their specific needs, providing flexibility in how data is managed and accessed.\\n\\n6. **Performance Optimization**: LlamaIndex includes performance optimization features to ensure that data retrieval is fast and efficient, even with large datasets.\\n\\n7. **Security**: It incorporates security measures to protect sensitive data and ensure that only authorized users can access certain information.\\n\\n8. **Interoperability**: The framework is designed to work seamlessly with various LLMs, making it a versatile tool for enhancing the capabilities of different language models.\\n\\n9. **User-Friendly Interface**: LlamaIndex offers a user-friendly interface that simplifies the process of connecting LLMs with external data sources, making it accessible to users with varying levels of technical expertise.\\n\\n10. **Documentation and Support**: Comprehensive documentation and support resources are available to help users get the most out of the framework and troubleshoot any issues that may arise.\\n\\nThese features make LlamaIndex a powerful tool for enhancing the functionality of LLMs by providing them with access to a wide range of external data sources.',\n",
              "   'reference': {'must_mention': ['features', 'capabilities']}},\n",
              "  '2bd8d76f-9b15-479b-8d7e-61777cfe4d70': {'input': {'question': 'What is the role of Hidden Markov Models in NLP?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a comprehensive explanation of the role of Hidden Markov Models in Natural Language Processing. It lists and explains several key applications of HMMs in NLP, including part-of-speech tagging, named entity recognition, speech recognition, machine translation, text segmentation, information extraction, spelling and grammar correction, and word sense disambiguation. \\n\\nThe submission is insightful as it not only lists the applications but also explains how HMMs are used in each of these applications. It also highlights the value of HMMs in modeling sequential data and capturing probabilistic relationships between different elements in a sequence.\\n\\nThe submission is appropriate as it directly answers the question and provides relevant information. It does not include any inappropriate or off-topic content.\\n\\nBased on these observations, the submission meets the criterion of helpfulness.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('5dd2c23d-ae42-4023-8024-7a002770831d'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is comprehensive and detailed, covering a wide range of applications of Hidden Markov Models (HMMs) in Natural Language Processing (NLP). The student correctly identifies that HMMs are used to model sequences of data, which is a common characteristic of language. They then go on to list and explain several key roles and applications of HMMs in NLP, including part-of-speech tagging, named entity recognition, speech recognition, machine translation, text segmentation, information extraction, spelling and grammar correction, and word sense disambiguation. The student's answer aligns with the context provided and does not contain any conflicting statements. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3f89a394-473c-465d-a494-07ddd20eb01d'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('97ca014d-23ce-48b5-9a1e-17450d4b43c9'), target_run_id=None)],\n",
              "   'execution_time': 7.7668,\n",
              "   'run_id': 'fb785d0c-8bf7-4e51-9d9b-3208a599dc8e',\n",
              "   'output': 'Hidden Markov Models (HMMs) play a significant role in Natural Language Processing (NLP) due to their ability to model sequences of data, which is a common characteristic of language. Here are some key roles and applications of HMMs in NLP:\\n\\n1. **Part-of-Speech Tagging**:\\n   - HMMs are used to assign parts of speech to each word in a sentence. The model treats the sequence of words as observable events and the sequence of parts of speech as hidden states. The goal is to find the most probable sequence of parts of speech given the sequence of words.\\n\\n2. **Named Entity Recognition (NER)**:\\n   - HMMs can be used to identify and classify named entities (like names of people, organizations, locations, etc.) in text. The model identifies the sequence of words that form named entities and classifies them into predefined categories.\\n\\n3. **Speech Recognition**:\\n   - In speech recognition, HMMs are used to model the sequence of spoken words. The observable events are the acoustic signals, and the hidden states are the words or phonemes. The model helps in decoding the most likely sequence of words from the acoustic signals.\\n\\n4. **Machine Translation**:\\n   - HMMs can be used in statistical machine translation to model the alignment between words in the source language and words in the target language. The hidden states represent the words in the target language, and the observable events are the words in the source language.\\n\\n5. **Text Segmentation**:\\n   - HMMs are used to segment text into meaningful units, such as sentences or topics. The model identifies the boundaries between segments based on the sequence of words.\\n\\n6. **Information Extraction**:\\n   - HMMs can be used to extract structured information from unstructured text. For example, extracting dates, prices, or other specific information from a document.\\n\\n7. **Spelling and Grammar Correction**:\\n   - HMMs can be used to detect and correct spelling and grammatical errors by modeling the sequence of characters or words and identifying unlikely sequences that may indicate errors.\\n\\n8. **Word Sense Disambiguation**:\\n   - HMMs can help in determining the correct sense of a word based on its context. The hidden states represent the possible senses of the word, and the observable events are the surrounding words.\\n\\nOverall, HMMs are valuable in NLP for their ability to model sequential data and capture the probabilistic relationships between different elements in a sequence. Despite the rise of more advanced models like neural networks, HMMs remain a foundational technique in the field.',\n",
              "   'reference': {'must_mention': ['sequence', 'Kurzweil']}},\n",
              "  '32ef5b73-c233-42ff-b0ff-0f9eea15bc8c': {'input': {'question': 'What are the differences between ReAct and Worker agents?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed comparison between ReAct and Worker agents, explaining their key differences in terms of cognitive architecture, decision-making process, flexibility, efficiency, and applications. It also provides a summary of the main points, which can be helpful for quick reference. \\n\\nThe information is presented in a clear and organized manner, making it easy to understand. The submission also provides insights into the strengths of each type of agent and the types of tasks or environments they are best suited for. \\n\\nThe submission is appropriate for the given input, as it directly addresses the question and provides a comprehensive answer. \\n\\nBased on this analysis, the submission meets the criterion of being helpful, insightful, and appropriate. \\n\\nTherefore, the answer is:\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1241fd95-1261-490f-bee3-ad6f30373e36'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer provides a detailed comparison between ReAct and Worker agents. The student correctly identifies the key differences between the two types of agents, including their cognitive architecture, decision-making process, flexibility, efficiency, and applications. The student also provides a summary that accurately encapsulates the main differences between ReAct and Worker agents. The student's answer aligns with the context provided, which asks for the differences between the two types of agents. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('fc7ab654-8e7a-48f6-88e2-aeaa08caaa81'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('e241d13c-50b3-4b29-b1ed-9c3eac936431'), target_run_id=None)],\n",
              "   'execution_time': 5.597917,\n",
              "   'run_id': '184c2ff6-c750-4375-96ea-ccd3a767b37e',\n",
              "   'output': 'ReAct (Reasoning and Acting) and Worker agents are two different paradigms in the field of artificial intelligence and autonomous systems. Here are the key differences between them:\\n\\n### ReAct (Reasoning and Acting) Agents:\\n1. **Cognitive Architecture**: ReAct agents are designed with a cognitive architecture that allows them to reason about their actions before executing them. This involves a deliberative process where the agent considers various options and their potential outcomes.\\n\\n2. **Decision-Making Process**: The decision-making process in ReAct agents is typically more complex and involves higher-level cognitive functions such as planning, problem-solving, and learning from past experiences.\\n\\n3. **Flexibility**: ReAct agents are generally more flexible and adaptable to changing environments because they can reason about new situations and modify their behavior accordingly.\\n\\n4. **Applications**: These agents are often used in complex, dynamic environments where decision-making requires a high level of intelligence, such as in robotics, autonomous vehicles, and advanced AI systems.\\n\\n### Worker Agents:\\n1. **Reactive Architecture**: Worker agents, on the other hand, are usually designed with a reactive architecture. They respond to stimuli or inputs from their environment in a pre-defined manner without extensive reasoning or deliberation.\\n\\n2. **Decision-Making Process**: The decision-making process in Worker agents is straightforward and rule-based. They follow a set of predefined rules or behaviors to respond to specific inputs or situations.\\n\\n3. **Efficiency**: Worker agents are generally more efficient in terms of computational resources because they do not engage in complex reasoning processes. They are designed to perform specific tasks quickly and reliably.\\n\\n4. **Applications**: These agents are often used in environments where tasks are repetitive and well-defined, such as in manufacturing, simple automation systems, and certain types of service robots.\\n\\n### Summary:\\n- **ReAct Agents**: Focus on reasoning and deliberation, suitable for complex and dynamic environments, more flexible and adaptable.\\n- **Worker Agents**: Focus on reactive and rule-based responses, suitable for repetitive and well-defined tasks, more efficient in terms of computational resources.\\n\\nBoth types of agents have their own strengths and are chosen based on the specific requirements of the task or environment they are designed to operate in.',\n",
              "   'reference': {'must_mention': ['differences', 'approaches']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8t-4FISCEh"
      },
      "source": [
        "We're going to add a custom helpfulness check here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!\n",
        "\n",
        "\n",
        "- The function `check_helpfulness` takes a dictionary called `state` which contains the conversation history as its parameter.\n",
        "- It extracts the first message and the last message from the conversation, storing them in `initial_query` and `final_response`.\n",
        "- Checks if the conversation has more than 10 messages; if yes, it returns \"END\" to terminate the processing early.\n",
        "- Defines a prompt template that asks a model to assess whether the final response was extremely helpful or not, using 'Y' for yes and 'N' for no.\n",
        "- creates `PromptTemplate` object, preparing it for interaction with a language model.\n",
        "- Initializes a GPT-4 model within the `ChatOpenAI` framework to process the prompt.\n",
        "- Constructs a pipeline (`helpfulness_chain`) linking the prompt template, the GPT-4 model, and a string output parser to handle the model's response.\n",
        "- Invokes the pipeline with the content of the initial and final messages, prompting the model to evaluate the helpfulness of the conversation's final response.\n",
        "- If the model's response includes \"Y\", it prints \"Helpful!\" and returns \"end\", indicating a positive evaluation, otherwise it print \"not helpful\" and return continue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "### Create instance of the StateGraph and then adding nodes to that object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### Setting the Entry Point of the StateGraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### Add conditional edges for should continue and check_helpfullnes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    check_helpfulness,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### Add edge and connection between action and agent node"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### Compile our graph/agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### test our retriver"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "72096ca6-e78e-475a-dde5-fe076c5b776d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpful!\n",
            "Initial Query: Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\n",
            "\n",
            "\n",
            "Agent Response: Let's break down each of these terms and individuals:\n",
            "\n",
            "### LoRA (Low-Rank Adaptation)\n",
            "LoRA stands for Low-Rank Adaptation. It is a technique used in machine learning, particularly in the context of fine-tuning large pre-trained models. The main idea behind LoRA is to adapt a pre-trained model to a new task by introducing a low-rank decomposition of the weight updates. This allows for efficient fine-tuning with fewer parameters, which can be particularly useful when dealing with large models that are computationally expensive to train.\n",
            "\n",
            "### Tim Dettmers\n",
            "Tim Dettmers is a researcher known for his work in the field of machine learning, particularly in the areas of deep learning and natural language processing. He has contributed to various advancements in model compression, efficient training techniques, and the application of machine learning to different domains. His work often focuses on making large-scale machine learning models more accessible and efficient.\n",
            "\n",
            "### Attention\n",
            "Attention is a mechanism used in neural networks, particularly in the context of sequence modeling tasks such as natural language processing. The attention mechanism allows the model to focus on different parts of the input sequence when making predictions. This is particularly useful in tasks like machine translation, where the model needs to align words in the source language with words in the target language.\n",
            "\n",
            "The most well-known form of attention is the \"self-attention\" mechanism used in Transformer models. In self-attention, each word in the input sequence is compared with every other word to compute a set of attention weights. These weights determine how much focus the model should place on each word when generating the output. This allows the model to capture long-range dependencies and relationships within the input sequence.\n",
            "\n",
            "Would you like more detailed information on any of these topics?\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "1c29765b-42fc-449f-dce4-62dcb747a03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpful!\n",
            "Initial Query: What is prompt engineering and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Prompt engineering is a technique used primarily in the field of artificial intelligence (AI) and natural language processing (NLP). It involves crafting specific inputs (prompts) to guide the behavior and responses of AI models, particularly large language models like GPT-3, GPT-4, and others. The goal is to elicit desired outputs from the model by carefully designing the input text.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "1. **Input Design**: Creating prompts that are clear, concise, and structured in a way that the AI can understand and respond to effectively.\n",
            "2. **Task Specification**: Defining the task or question in a manner that the AI can interpret correctly.\n",
            "3. **Context Provision**: Providing sufficient context within the prompt to help the AI generate relevant and accurate responses.\n",
            "4. **Iterative Refinement**: Continuously refining prompts based on the AI's responses to improve accuracy and relevance.\n",
            "\n",
            "### Emergence of Prompt Engineering:\n",
            "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The ability of these models to generate human-like text based on prompts led to a growing interest in optimizing these prompts to achieve specific outcomes. Researchers and practitioners began to explore various techniques to improve the performance of AI models through better prompt design.\n",
            "\n",
            "The concept of prompt engineering, however, has roots in earlier AI and NLP research, where the design of input queries and instructions has always been crucial for effective machine learning and AI performance. The term itself gained more traction and formal recognition with the rise of powerful language models and their applications in diverse fields.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is RAG and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: RAG stands for Retrieval-Augmented Generation. It is a framework that combines the strengths of retrieval-based and generation-based models to improve the performance of natural language processing tasks, particularly in the context of question answering and information retrieval.\n",
            "\n",
            "### Key Components of RAG:\n",
            "1. **Retriever**: This component is responsible for fetching relevant documents or passages from a large corpus based on the input query.\n",
            "2. **Generator**: This component generates a coherent and contextually appropriate response based on the retrieved documents.\n",
            "\n",
            "### How RAG Works:\n",
            "1. **Query Input**: A user inputs a query.\n",
            "2. **Document Retrieval**: The retriever fetches relevant documents or passages from a pre-indexed corpus.\n",
            "3. **Response Generation**: The generator uses the retrieved documents to generate a response that is both relevant and contextually appropriate.\n",
            "\n",
            "### Advantages of RAG:\n",
            "- **Improved Accuracy**: By leveraging relevant documents, the model can generate more accurate and contextually appropriate responses.\n",
            "- **Scalability**: The retrieval component can handle large corpora, making it scalable for various applications.\n",
            "- **Flexibility**: It can be fine-tuned for specific domains or tasks, enhancing its versatility.\n",
            "\n",
            "### When Did RAG Break Onto the Scene?\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The paper demonstrated the effectiveness of RAG in various knowledge-intensive tasks, such as open-domain question answering and fact-checking.\n",
            "\n",
            "Would you like more detailed information or specific aspects of RAG?\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is fine-tuning and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large, general dataset, allowing it to perform well on a new, often smaller, and more specific dataset with relatively less training time and data.\n",
            "\n",
            "### Key Steps in Fine-Tuning:\n",
            "1. **Pre-training**: A model is trained on a large, general dataset. For example, a language model might be trained on a vast corpus of text from the internet.\n",
            "2. **Fine-tuning**: The pre-trained model is then further trained (fine-tuned) on a smaller, task-specific dataset. This helps the model adapt to the nuances of the new task.\n",
            "\n",
            "### Benefits of Fine-Tuning:\n",
            "- **Efficiency**: Reduces the amount of data and computational resources needed compared to training a model from scratch.\n",
            "- **Performance**: Often leads to better performance on specific tasks because the model starts with a strong foundation of general knowledge.\n",
            "\n",
            "### Historical Context:\n",
            "Fine-tuning became particularly prominent with the advent of deep learning and the development of large pre-trained models. Some key milestones include:\n",
            "\n",
            "- **2014**: The concept of transfer learning, which underpins fine-tuning, gained traction with the success of models like AlexNet and VGG in computer vision tasks.\n",
            "- **2018**: The release of BERT (Bidirectional Encoder Representations from Transformers) by Google marked a significant breakthrough in natural language processing (NLP). BERT demonstrated the power of fine-tuning by achieving state-of-the-art results on a variety of NLP tasks.\n",
            "- **2019**: OpenAI's GPT-2 further showcased the potential of fine-tuning in generating coherent and contextually relevant text.\n",
            "\n",
            "Fine-tuning has since become a standard practice in the development of machine learning models, especially in fields like NLP, computer vision, and speech recognition.\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is LLM-based agents and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: LLM-based agents refer to systems or applications that leverage Large Language Models (LLMs) to perform a variety of tasks. These tasks can range from natural language understanding and generation to more complex activities like decision-making, problem-solving, and interacting with users in a conversational manner. LLM-based agents are typically built on advanced neural network architectures, such as transformers, which are trained on vast amounts of text data to understand and generate human-like text.\n",
            "\n",
            "### Key Characteristics of LLM-based Agents:\n",
            "1. **Natural Language Understanding (NLU):** They can comprehend and interpret human language in a way that is contextually relevant.\n",
            "2. **Natural Language Generation (NLG):** They can generate coherent and contextually appropriate text based on the input they receive.\n",
            "3. **Conversational Abilities:** They can engage in dialogue with users, answering questions, providing information, and even performing tasks.\n",
            "4. **Adaptability:** They can be fine-tuned for specific applications, such as customer service, content creation, or personal assistants.\n",
            "\n",
            "### Historical Context:\n",
            "The concept of using machine learning for language tasks has been around for decades, but the breakthrough with LLMs can be traced to a few key developments:\n",
            "\n",
            "1. **Transformer Architecture (2017):** The introduction of the transformer model by Vaswani et al. in the paper \"Attention is All You Need\" was a significant milestone. This architecture enabled more efficient training of large models and improved performance on a variety of language tasks.\n",
            "\n",
            "2. **GPT Series by OpenAI:**\n",
            "   - **GPT-1 (2018):** The first Generative Pre-trained Transformer (GPT) model demonstrated the potential of pre-training on a large corpus of text followed by fine-tuning on specific tasks.\n",
            "   - **GPT-2 (2019):** This model gained widespread attention due to its impressive ability to generate coherent and contextually relevant text, leading to discussions about the implications of such powerful models.\n",
            "   - **GPT-3 (2020):** With 175 billion parameters, GPT-3 showcased unprecedented capabilities in natural language understanding and generation, making it a cornerstone for many LLM-based applications.\n",
            "\n",
            "3. **BERT by Google (2018):** The Bidirectional Encoder Representations from Transformers (BERT) model introduced a new approach to pre-training language models, focusing on bidirectional context, which significantly improved performance on various NLU tasks.\n",
            "\n",
            "### Breakthrough and Adoption:\n",
            "The adoption of LLM-based agents accelerated with the release of GPT-3 in 2020. Its ability to perform a wide range of tasks with minimal fine-tuning made it a versatile tool for developers and businesses. Since then, LLMs have been integrated into numerous applications, including chatbots, virtual assistants, content generation tools, and more.\n",
            "\n",
            "In summary, LLM-based agents broke onto the scene primarily with the development and release of transformer-based models, particularly the GPT series by OpenAI, starting around 2018 and gaining significant traction by 2020.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print_messages(messages)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.9 ('llmops-w5')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "c9e5523a24c5cd4eda77ee493767e4015a2e2bbdd0ec52ee3dcc95a5bf09d0e7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
