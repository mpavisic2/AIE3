{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wa8ykQk92aLX"
      },
      "source": [
        "# Evaluation of RAG Using Ragas\n",
        "\n",
        "In the following notebook we'll explore how to evaluate RAG pipelines using a powerful open-source tool called \"Ragas\". This will give us tools to evaluate component-wise metrics, as well as end-to-end metrics about the performance of our RAG pipelines.\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- 🤝 Breakout Room Part #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating a simple RAG pipeline with [LangChain v0.2.0](https://python.langchain.com/v0.2/docs/versions/v0_2/)\n",
        "  4. Synthetic Dataset Generation for Evaluation using the [Ragas](https://github.com/explodinggradients/ragas) framework.\n",
        "  \n",
        "\n",
        "- 🤝 Breakout Room Part #2:\n",
        "  1. Evaluating our pipeline with Ragas\n",
        "  3. Making Adjustments to our RAG Pipeline\n",
        "  4. Evaluating our Adjusted pipeline against our baseline\n",
        "  5. Testing OpenAI's Claim\n",
        "\n",
        "The only way to get started is to get started - so let's grab our dependencies for the day!\n",
        "\n",
        "> NOTE: Using this notebook as presented will occur a charge of ~$3USD from OpenAI usage. Most of this cost is produced by the Synthetic Data Generation step - if you want to reduce costs, please use the provided commented code to leverage `GPT-3.5-Turbo` as the `critic_llm`!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h4yh6f7q9uN"
      },
      "source": [
        "## Motivation\n",
        "\n",
        "A claim, made by OpenAI, is that their `text-embedding-3-small` is better (generally) than their `text-embedding-ada-002` model.\n",
        "\n",
        "Here's some passages from their [blog](https://openai.com/blog/new-embedding-models-and-api-updates) about the `text-embedding-3` release:\n",
        "\n",
        "> `text-embedding-3-small` is our new highly efficient embedding model and provides a significant upgrade over its predecessor, the `text-embedding-ada-002` model...\n",
        "\n",
        "> **Stronger performance.** Comparing `text-embedding-ada-002` to `text-embedding-3-small`, the average score on a commonly used benchmark for multi-language retrieval ([MIRACL](https://github.com/project-miracl/miracl)) has increased from 31.4% to 44.0%, while the average score on a commonly used benchmark for English tasks ([MTEB](https://github.com/embeddings-benchmark/mteb)) has increased from 61.0% to 62.3%.\n",
        "\n",
        "Well, with a library like Ragas - we can put that claim to the test!\n",
        "\n",
        "If what they claim is true - we should see an increase on related metrics by using the new embedding model!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAH1znJ2pIp3"
      },
      "source": [
        "# 🤝 Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpkXAmMZpLhm"
      },
      "source": [
        "## Task 1: Installing Required Libraries\n",
        "\n",
        "A reminder that one of the [key features](https://blog.langchain.dev/langchain-v0-1-0/) of LangChain v0.1.0 is the compartmentalization of the various LangChain ecosystem packages!\n",
        "\n",
        "So let's begin grabbing all of our LangChain related packages!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BN13TZlSCv4",
        "outputId": "51d9c154-af83-42b2-ce72-9656729ecb9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.6/974.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.8/321.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m327.4/327.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.1/127.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "! pip install -U -q langchain langchain-openai langchain_core langchain-community langchainhub openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fm7gXsD6pqG0"
      },
      "source": [
        "We'll also get the \"star of the show\" today, which is Ragas!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvAvDNWBpjQ1",
        "outputId": "20ff8c89-11db-4071-b0a0-6b9bfc0e215f"
      },
      "outputs": [],
      "source": [
        "! pip install -qU ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9q6Z9oTpw3X"
      },
      "source": [
        "We'll be leveraging [QDrant](https://qdrant.tech/) again as our LangChain `VectorStore`.\n",
        "\n",
        "We'll also install `pymupdf` and its dependencies which will allow us to load PDFs using the `PyMuPDFLoader` in the `langchain-community` package!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAJK95napn8I",
        "outputId": "58f04109-385b-44c7-d3cb-4547d8acaea1"
      },
      "outputs": [],
      "source": [
        "#! pip install -qU qdrant-client pymupdf pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_C2JvG1qO3h"
      },
      "source": [
        "## Task 2: Set Environment Variables\n",
        "\n",
        "Let's set up our OpenAI API key so we can leverage their API later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Lhqp5rUThG-",
        "outputId": "97cb739d-66b4-4476-ca04-b6257004178f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass\n",
        "\n",
        "openai.api_key = getpass(\"Please provide your OpenAI Key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFbWNvo3rZ4H"
      },
      "source": [
        "## Task 3: Creating a Simple RAG Pipeline with LangChain v0.1.0\n",
        "\n",
        "Building on what we learned last week, we'll be leveraging LangChain v0.1.0 and LCEL to build a simple RAG pipeline that we can baseline with Ragas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV_BOewX8CW0"
      },
      "source": [
        "## Building our RAG pipeline\n",
        "\n",
        "Let's review the basic steps of RAG again:\n",
        "\n",
        "- Create an Index\n",
        "- Use retrieval to obtain pieces of context from our Index that are similar to our query\n",
        "- Use a LLM to generate responses based on the retrieved context\n",
        "\n",
        "Let's get started by creating our index.\n",
        "\n",
        "> NOTE: We're going to start leaning on the term \"index\" to refer to our `VectorStore`, `VectorDatabase`, etc. We can think of \"index\" as the catch-all term, whereas `VectorStore` and the like relate to the specific technologies used to create, store, and interact with the index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VDGJdxCJEVc"
      },
      "source": [
        "### Creating an Index\n",
        "\n",
        "You'll notice that the largest changes (outside of some import changes) are that our old favourite chains are back to being bundled in an easily usable abstraction.\n",
        "\n",
        "We can still create custom chains using LCEL - but we can also be more confident that our pre-packaged chains are creating using LCEL under the hood."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmFFThawK8lO"
      },
      "source": [
        "#### Loading Data\n",
        "\n",
        "Let's start by loading some data!\n",
        "\n",
        "> NOTE: You'll notice that we're using a document loader from the community package of LangChain. This is part of the v0.2.0 changes that make the base (`langchain-core`) package remain lightweight while still providing access to some of the more powerful community integrations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DTDNFXaBSO2j"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader(\n",
        "    \"https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf\",\n",
        ")\n",
        "\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3dJYlBCIX_p",
        "outputId": "1383c5b7-bb72-49ea-d323-fcd9eed48d60"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf',\n",
              " 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf',\n",
              " 'page': 0,\n",
              " 'total_pages': 195,\n",
              " 'format': 'PDF 1.3',\n",
              " 'title': 'The Pmarca Blog Archives',\n",
              " 'author': '',\n",
              " 'subject': '',\n",
              " 'keywords': '',\n",
              " 'creator': '',\n",
              " 'producer': 'Mac OS X 10.10 Quartz PDFContext',\n",
              " 'creationDate': \"D:20150110020418Z00'00'\",\n",
              " 'modDate': \"D:20150110020418Z00'00'\",\n",
              " 'trapped': ''}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "documents[0].metadata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQUl3sbZK4_1"
      },
      "source": [
        "#### Transforming Data\n",
        "\n",
        "Now that we've got our single document - let's split it into smaller pieces so we can more effectively leverage it with our retrieval chain!\n",
        "\n",
        "We'll start with the classic: `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6Nt2E1xnLNgr"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 200,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "documents = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ilzwQxhiLcVV"
      },
      "source": [
        "Let's confirm we've split our document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wRw6a4aLfWh",
        "outputId": "a707bbf6-6338-45fc-a75e-86d693dfe2c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1864"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ93HkYcMJwW"
      },
      "source": [
        "#### Loading OpenAI Embeddings Model\n",
        "\n",
        "We'll need a process by which we can convert our text into vectors that allow us to compare to our query vector.\n",
        "\n",
        "Let's use OpenAI's `text-embedding-ada-002` for this task!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JU6CrDVZMgKe"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-ada-002\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVtZR9JPLtR4"
      },
      "source": [
        "#### Creating a QDrant VectorStore\n",
        "\n",
        "Now that we have documents - we'll need a place to store them alongside their embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "978TWiCtMA0B"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "\n",
        "qdrant_vector_store = Qdrant.from_documents(\n",
        "    documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"PMarca Blogs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk50NmrMDlWu"
      },
      "source": [
        "####❓ Question #1:\n",
        "\n",
        "List out a few of the techniques that Qdrant uses that make it performant.\n",
        "\n",
        "> NOTE: Check the [documentation](https://qdrant.tech/documentation/overview/) for more information about QDrant!\n",
        "\n",
        "**Answer**\n",
        "\n",
        "\n",
        "HNSW Graphs: These are used for efficient indexing. They allow Qdrant to sift through high-dimensional data quickly, reducing the time it takes to find relevant results.\n",
        "\n",
        "Product Quantization: This is a technique used to compress vectors. It ensures that the engine is not just fast but also memory-efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7ht6bJX9PAY"
      },
      "source": [
        "#### Creating a Retriever\n",
        "\n",
        "To complete our index, all that's left to do is expose our vectorstore as a retriever - which we can do the same way we would in previous version of LangChain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xne8P5dQTUiR"
      },
      "outputs": [],
      "source": [
        "retriever = qdrant_vector_store.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO_DFBVKNvNm"
      },
      "source": [
        "#### Testing our Retriever\n",
        "\n",
        "Now that we've gone through the trouble of creating our retriever - let's see it in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "I9_ONxpnN0n6"
      },
      "outputs": [],
      "source": [
        "retrieved_documents = retriever.invoke(\"What is a rule of thumb for selecting an industry to invest in?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Za12yt4OBy1",
        "outputId": "6dfa1ae5-8198-49a1-b213-96b34a1a5147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='the existing order — and make sure that those forces of change\\nhave a reasonable chance at succeeding.\\nSecond rule of thumb:\\nOnce you have picked an industry, get right to the center of it' metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 125, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': '2ffd8ea39ce24a139c8c22c39641095a', '_collection_name': 'PMarca Blogs'}\n",
            "page_content='Third rule:\\nIn a rapidly changing Held like technology, the best place to\\nget experience when you’re starting out is in younger, high-\\ngrowth companies.' metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 127, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': '4e3dfeea0b1c4812959232ab94f46e54', '_collection_name': 'PMarca Blogs'}\n",
            "page_content='where the great opportunities can be found.\\nApply this rule when selecting which company to go to. Go to\\nthe company where all the action is happening.' metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 125, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': '8e2c53642d6548fa8103248e810cb88b', '_collection_name': 'PMarca Blogs'}\n",
            "page_content='growth companies.\\n(This is not necessarily true in older and more established\\nindustries, but those aren’t the industries we’re talking about.)' metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 127, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': 'cbe3cd855e1e42a2b9e1c51256ddb2bb', '_collection_name': 'PMarca Blogs'}\n"
          ]
        }
      ],
      "source": [
        "for doc in retrieved_documents:\n",
        "  print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8MKsT6JTgCU"
      },
      "source": [
        "### Creating a RAG Chain\n",
        "\n",
        "Now that we have the \"R\" in RAG taken care of - let's look at creating the \"AG\"!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs7qBLaEQEic"
      },
      "source": [
        "#### Creating a Prompt Template\n",
        "\n",
        "There are a few different ways we could create our prompt template - we could create a custom template, as seen in the code below, or we could simply pull a prompt from the prompt hub! Let's look at an example of that!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.20-py3-none-any.whl.metadata (659 bytes)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages (from langchainhub) (23.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages (from langchainhub) (2.32.3)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20240622-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages (from requests<3,>=2->langchainhub) (2024.6.2)\n",
            "Downloading langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\n",
            "Downloading types_requests-2.32.0.20240622-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: types-requests, langchainhub\n",
            "Successfully installed langchainhub-0.1.20 types-requests-2.32.0.20240622\n"
          ]
        }
      ],
      "source": [
        "! pip install langchainhub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eRCq_OKUQbKk"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "\n",
        "retrieval_qa_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FziTftV5Q1H-",
        "outputId": "21189f0e-4b5d-4146-8071-eb0fff4a6f13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer any use questions based solely on the context below:\n",
            "\n",
            "<context>\n",
            "{context}\n",
            "</context>\n"
          ]
        }
      ],
      "source": [
        "print(retrieval_qa_prompt.messages[0].prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyq88IPFRGoT"
      },
      "source": [
        "As you can see - the prompt template is simple (and has a small error) - so we'll create our own to be a bit more specific!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ijSNkTAjTsep"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYHnPaXl-cvJ"
      },
      "source": [
        "#### Setting Up our Basic QA Chain\n",
        "\n",
        "Now we can instantiate our basic RAG chain!\n",
        "\n",
        "We'll use LCEL directly just to see an example of it - but you could just as easily use an abstraction here to achieve the same goal!\n",
        "\n",
        "We'll also ensure to pass-through our context - which is critical for RAGAS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "-TsjUWjbUfbW"
      },
      "outputs": [],
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "primary_qa_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MgAa9JwBuJx"
      },
      "source": [
        "####🏗️ Activity #1:\n",
        "\n",
        "Describe the pipeline shown above in simple terms. You can include a diagram if desired.\n",
        "\n",
        "**Answer**\n",
        "\n",
        "* we are creating llm, prompt template and retriver\n",
        "* then we wrap everything in qa chain\n",
        "* in first step we provide question and retriver\n",
        "* output of that is context and question key\n",
        "* at the end we are just collecting context from previous step for context key output\n",
        "* and response is generated by chaining prompt template (collect context and question data from provided keys)  with llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO69de-F-oMD"
      },
      "source": [
        "Let's test it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FS5NxC6UyU2",
        "outputId": "db9953a2-758d-4723-cd95-e980a47715d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get right to the center of the industry.\n"
          ]
        }
      ],
      "source": [
        "question = \"What is a rule of thumb for selecting an industry to invest in?\"\n",
        "\n",
        "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "\n",
        "print(result[\"response\"].content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIuHVGPOO9P2",
        "outputId": "24ce3524-9284-4eea-f78b-4329a615d321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I don't know.\n",
            "[Document(page_content='ask if you can call them again if things change.\\nTrust me — they’d much rather be saying “yes” than “no” —\\nthey need all the good investments they can get.\\nSecond, consider the environment.', metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 15, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': 'c6b24d6b32c4451089069dcab9304d1d', '_collection_name': 'PMarca Blogs'}), Document(page_content='watching carefully — if everyone agrees right up front that\\nwhatever you are doing makes total sense, it probably isn’t a new\\nand radical enough idea to justify a new company.', metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 152, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': '80c2a4446bfa404e9a2cc89c215ac9c3', '_collection_name': 'PMarca Blogs'}), Document(page_content='Third rule:\\nIn a rapidly changing Held like technology, the best place to\\nget experience when you’re starting out is in younger, high-\\ngrowth companies.', metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 127, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': '4e3dfeea0b1c4812959232ab94f46e54', '_collection_name': 'PMarca Blogs'}), Document(page_content='the existing order — and make sure that those forces of change\\nhave a reasonable chance at succeeding.\\nSecond rule of thumb:\\nOnce you have picked an industry, get right to the center of it', metadata={'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 125, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': '', '_id': '2ffd8ea39ce24a139c8c22c39641095a', '_collection_name': 'PMarca Blogs'})]\n"
          ]
        }
      ],
      "source": [
        "question = \"What did Pink Floyd have to say about how to proceed when investing in a new industry?\"\n",
        "\n",
        "result = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "\n",
        "print(result[\"response\"].content)\n",
        "print(result[\"context\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-XYZueEP42k"
      },
      "source": [
        "We can already see that there are some improvements we could make here.\n",
        "\n",
        "For now, let's switch gears to RAGAS to see how we can leverage that tool to provide us insight into how our pipeline is performing!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOECHyzHRqDw"
      },
      "source": [
        "## Task 4: Synthetic Dataset Generation for Evaluation using Ragas\n",
        "\n",
        "Ragas is a powerful library that lets us evaluate our RAG pipeline by collecting input/output/context triplets and obtaining metrics relating to a number of different aspects of our RAG pipeline.\n",
        "\n",
        "We'll be evaluating on every core metric today, but in order to do that - we'll need to create a test set. Luckily for us, Ragas can do that directly!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqXQ0jweWJOu"
      },
      "source": [
        "### Synthetic Test Set Generation\n",
        "\n",
        "We can leverage Ragas' [`Synthetic Test Data generation`](https://docs.ragas.io/en/stable/concepts/testset_generation.html) functionality to generate our own synthetic QC pairs - as well as a synthetic ground truth - quite easily!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "nVk5SlU9znXe"
      },
      "outputs": [],
      "source": [
        "loader = PyMuPDFLoader(\n",
        "    \"https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf\",\n",
        ")\n",
        "\n",
        "eval_documents = loader.load()\n",
        "\n",
        "text_splitter_eval = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 600,\n",
        "    chunk_overlap = 50\n",
        ")\n",
        "\n",
        "eval_documents = text_splitter_eval.split_documents(eval_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7rOQkxhzrq3"
      },
      "source": [
        "####❓ Question #2:\n",
        "\n",
        "Why is it important to split our documents using different parameters when creating our synthetic data?\n",
        "\n",
        "several reasons:\n",
        "\n",
        "Rag evaluation requires a diverse set of questions and scenarios to assess the model's generation quality comprehensively, and for that we need bigger chunks!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiAPYw-hz-zo",
        "outputId": "fc8c6829-5c53-4eb1-8407-1545f5a7d023"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "624"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(eval_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYCrVMW9Blda"
      },
      "source": [
        "\n",
        "> NOTE: 🛑 Using this notebook as presented will occur a charge of ~$3USD from OpenAI usage. Most of this cost is produced by the Synthetic Data Generation step - if you want to reduce costs, please use the provided commented code to leverage GPT-3.5-Turbo as the critic_llm. If you're attempting to create a lot of samples please be aware of cost, as well as rate limits. 🛑"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "generator_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\")\n",
        "critic_llm = ChatOpenAI(model=\"gpt-3.5-turbo\") # <--- If you don't have GPT-4 access, or to reduce cost/rate limiting issues.\n",
        "# critic_llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "generator = TestsetGenerator.from_langchain(\n",
        "    generator_llm,\n",
        "    critic_llm,\n",
        "    embeddings\n",
        ")\n",
        "\n",
        "distributions = {\n",
        "    simple: 0.5,\n",
        "    multi_context: 0.4,\n",
        "    reasoning: 0.1\n",
        "}\n",
        "\n",
        "testset = generator.generate_with_langchain_docs(eval_documents, 20, distributions, is_async = False)\n",
        "testset.to_pandas()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOIGT0XLz8ze"
      },
      "source": [
        "####❓ Question #3:\n",
        "\n",
        "`{simple: 0.5, reasoning: 0.25, multi_context: 0.25}`\n",
        "\n",
        "What exactly does this mapping refer to?\n",
        "\n",
        "> NOTE: Check out the Ragas documentation on this generation process [here](https://docs.ragas.io/en/stable/concepts/testset_generation.html).\n",
        "\n",
        "\n",
        "**Answer**\n",
        "\n",
        "Reasoning: Rewrite the question in a way that enhances the need for reasoning to answer it effectively.\n",
        "\n",
        "Conditioning: Modify the question to introduce a conditional element, which adds complexity to the question.\n",
        "\n",
        "Multi-Context: Rephrase the question in a manner that necessitates information from multiple related sections or chunks to formulate an answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MemL406rUzBu"
      },
      "source": [
        "Let's look at the output and see what we can learn about it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaCDdImVU15s",
        "outputId": "31efbb94-f09d-4d50-8c6e-59202aaeb5c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataRow(question='Why does the author believe that intelligence is overrated?', contexts=['Criteria 7rst\\nLots of people will tell you to hire for intelligence.\\nEspecially in this industry.\\nYou will read, hire the smartest people out there and your com-\\npany’s success is all but guaranteed.\\nI think intelligence, per se, is highly overrated.\\nSpeciXcally, I am unaware of any actual data that shows a cor-'], ground_truth='The author believes that intelligence is overrated because they are unaware of any actual data that shows a correlation between intelligence and company success.', evolution_type='simple', metadata=[{'source': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'file_path': 'https://d1lamhf6l6yk6d.cloudfront.net/uploads/2021/08/The-pmarca-Blog-Archives.pdf', 'page': 73, 'total_pages': 195, 'format': 'PDF 1.3', 'title': 'The Pmarca Blog Archives', 'author': '', 'subject': '', 'keywords': '', 'creator': '', 'producer': 'Mac OS X 10.10 Quartz PDFContext', 'creationDate': \"D:20150110020418Z00'00'\", 'modDate': \"D:20150110020418Z00'00'\", 'trapped': ''}])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "testset.test_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrPsVwUAWFWB"
      },
      "source": [
        "### Generating Responses with RAG Pipeline\n",
        "\n",
        "Now that we have some QC pairs, and some ground truths, let's evaluate our RAG pipeline using Ragas.\n",
        "\n",
        "The process is, again, quite straightforward - thanks to Ragas and LangChain!\n",
        "\n",
        "Let's start by extracting our questions and ground truths from our create testset.\n",
        "\n",
        "We can start by converting our test dataset into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "frvzu1YxX8kY"
      },
      "outputs": [],
      "source": [
        "test_df = testset.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GFKMIY8IZU8m",
        "outputId": "ed137f4f-df2c-41fa-d868-802d30076ea0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>evolution_type</th>\n",
              "      <th>metadata</th>\n",
              "      <th>episode_done</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Why does the author believe that intelligence ...</td>\n",
              "      <td>[Criteria 7rst\\nLots of people will tell you t...</td>\n",
              "      <td>The author believes that intelligence is overr...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How important is it for a startup to focus on ...</td>\n",
              "      <td>[developing a large market, as opposed to Xght...</td>\n",
              "      <td>Focusing on developing a large market is impor...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What impact does the presence of a billion peo...</td>\n",
              "      <td>[billion people online now. That is a very lar...</td>\n",
              "      <td>The presence of a billion people online has le...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What are the consequences of not raising enoug...</td>\n",
              "      <td>[Here’s why you shouldn’t do that:\\nWhat are t...</td>\n",
              "      <td>Not raising enough money risks the survival of...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does a deal with a big company typically i...</td>\n",
              "      <td>[always be ready to have the conversation just...</td>\n",
              "      <td>A deal with a big company typically does not g...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What industry is Washington DC known for in te...</td>\n",
              "      <td>[most interesting opportunity available — the ...</td>\n",
              "      <td>Washington DC is known for the politics industry.</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What are some common layers of risk for a high...</td>\n",
              "      <td>[as if it’s an onion. Just like you peel an on...</td>\n",
              "      <td>Here are some common layers of risk for a high...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How are high expectations beneficial in a high...</td>\n",
              "      <td>[There are a bunch of great things that you ge...</td>\n",
              "      <td>High expectations in a high-growth company can...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the quality of output in a creative c...</td>\n",
              "      <td>[becomes irrelevant to determining the success...</td>\n",
              "      <td>Quality of output in a creative career does no...</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What are the criteria for evaluating candidate...</td>\n",
              "      <td>[How to hire the best people you've\\never work...</td>\n",
              "      <td>nan</td>\n",
              "      <td>simple</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How does perceived risk impact decision-making...</td>\n",
              "      <td>[sonal relationships within the group — so the...</td>\n",
              "      <td>The desire to be liked in a startup with multi...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What is the purpose of the \"Pending\" email fol...</td>\n",
              "      <td>[keep three standing email folders: Pending, R...</td>\n",
              "      <td>The purpose of the \"Pending\" email folder is t...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>What advantages does a technical degree offer ...</td>\n",
              "      <td>[•\\nPlus, technical degrees teach you how thin...</td>\n",
              "      <td>Technical degrees offer the advantage of teach...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>What is the correlation between startup team c...</td>\n",
              "      <td>[Let’s start by deXning terms.\\nThe caliber of...</td>\n",
              "      <td>nan</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>What factors should be considered when making ...</td>\n",
              "      <td>[the team, not good for the burn rate, and not...</td>\n",
              "      <td>The factors that should be considered when mak...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>What forms of chance exist and how do they rel...</td>\n",
              "      <td>[that chance is immune from human intervention...</td>\n",
              "      <td>The four forms of chance are Chance I and Chan...</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>How does the Liking/Loving Tendency affect ent...</td>\n",
              "      <td>[One very practical consequence of Liking/Lovi...</td>\n",
              "      <td>nan</td>\n",
              "      <td>multi_context</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>What is the purpose of temporary subfolders in...</td>\n",
              "      <td>[you can reply to a lot of messages with “I’m ...</td>\n",
              "      <td>The purpose of temporary subfolders in email o...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>What is the impact of the shift towards restri...</td>\n",
              "      <td>[of creating value. And new hires will by deXn...</td>\n",
              "      <td>The shift towards restricted stock and away fr...</td>\n",
              "      <td>reasoning</td>\n",
              "      <td>[{'source': 'https://d1lamhf6l6yk6d.cloudfront...</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             question  \\\n",
              "0   Why does the author believe that intelligence ...   \n",
              "1   How important is it for a startup to focus on ...   \n",
              "2   What impact does the presence of a billion peo...   \n",
              "3   What are the consequences of not raising enoug...   \n",
              "4   How does a deal with a big company typically i...   \n",
              "5   What industry is Washington DC known for in te...   \n",
              "6   What are some common layers of risk for a high...   \n",
              "7   How are high expectations beneficial in a high...   \n",
              "8   How does the quality of output in a creative c...   \n",
              "9   What are the criteria for evaluating candidate...   \n",
              "10  How does perceived risk impact decision-making...   \n",
              "11  What is the purpose of the \"Pending\" email fol...   \n",
              "12  What advantages does a technical degree offer ...   \n",
              "13  What is the correlation between startup team c...   \n",
              "14  What factors should be considered when making ...   \n",
              "15  What forms of chance exist and how do they rel...   \n",
              "16  How does the Liking/Loving Tendency affect ent...   \n",
              "17  What is the purpose of temporary subfolders in...   \n",
              "18  What is the impact of the shift towards restri...   \n",
              "\n",
              "                                             contexts  \\\n",
              "0   [Criteria 7rst\\nLots of people will tell you t...   \n",
              "1   [developing a large market, as opposed to Xght...   \n",
              "2   [billion people online now. That is a very lar...   \n",
              "3   [Here’s why you shouldn’t do that:\\nWhat are t...   \n",
              "4   [always be ready to have the conversation just...   \n",
              "5   [most interesting opportunity available — the ...   \n",
              "6   [as if it’s an onion. Just like you peel an on...   \n",
              "7   [There are a bunch of great things that you ge...   \n",
              "8   [becomes irrelevant to determining the success...   \n",
              "9   [How to hire the best people you've\\never work...   \n",
              "10  [sonal relationships within the group — so the...   \n",
              "11  [keep three standing email folders: Pending, R...   \n",
              "12  [•\\nPlus, technical degrees teach you how thin...   \n",
              "13  [Let’s start by deXning terms.\\nThe caliber of...   \n",
              "14  [the team, not good for the burn rate, and not...   \n",
              "15  [that chance is immune from human intervention...   \n",
              "16  [One very practical consequence of Liking/Lovi...   \n",
              "17  [you can reply to a lot of messages with “I’m ...   \n",
              "18  [of creating value. And new hires will by deXn...   \n",
              "\n",
              "                                         ground_truth evolution_type  \\\n",
              "0   The author believes that intelligence is overr...         simple   \n",
              "1   Focusing on developing a large market is impor...         simple   \n",
              "2   The presence of a billion people online has le...         simple   \n",
              "3   Not raising enough money risks the survival of...         simple   \n",
              "4   A deal with a big company typically does not g...         simple   \n",
              "5   Washington DC is known for the politics industry.         simple   \n",
              "6   Here are some common layers of risk for a high...         simple   \n",
              "7   High expectations in a high-growth company can...         simple   \n",
              "8   Quality of output in a creative career does no...         simple   \n",
              "9                                                 nan         simple   \n",
              "10  The desire to be liked in a startup with multi...  multi_context   \n",
              "11  The purpose of the \"Pending\" email folder is t...  multi_context   \n",
              "12  Technical degrees offer the advantage of teach...  multi_context   \n",
              "13                                                nan  multi_context   \n",
              "14  The factors that should be considered when mak...  multi_context   \n",
              "15  The four forms of chance are Chance I and Chan...  multi_context   \n",
              "16                                                nan  multi_context   \n",
              "17  The purpose of temporary subfolders in email o...      reasoning   \n",
              "18  The shift towards restricted stock and away fr...      reasoning   \n",
              "\n",
              "                                             metadata  episode_done  \n",
              "0   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "1   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "2   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "3   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "4   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "5   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "6   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "7   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "8   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "9   [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "10  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "11  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "12  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "13  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "14  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "15  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "16  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "17  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  \n",
              "18  [{'source': 'https://d1lamhf6l6yk6d.cloudfront...          True  "
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xAiXbVmLYSoC"
      },
      "outputs": [],
      "source": [
        "test_questions = test_df[\"question\"].values.tolist()\n",
        "test_groundtruths = test_df[\"ground_truth\"].values.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE5rfMLfbqKH"
      },
      "source": [
        "Now we'll generate responses using our RAG pipeline using the questions we've generated - we'll also need to collect our retrieved contexts for each question.\n",
        "\n",
        "We'll do this in a simple loop to see exactly what's happening!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9_AayvT1dAQN"
      },
      "outputs": [],
      "source": [
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for question in test_questions:\n",
        "  response = retrieval_augmented_qa_chain.invoke({\"question\" : question})\n",
        "  answers.append(response[\"response\"].content)\n",
        "  contexts.append([context.page_content for context in response[\"context\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opHaHmYDeBfC"
      },
      "source": [
        "Now we can wrap our information in a Hugging Face dataset for use in the Ragas library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "fY48YZITeHy-"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "response_dataset = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : answers,\n",
        "    \"contexts\" : contexts,\n",
        "    \"ground_truth\" : test_groundtruths\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmeVvQaZeogE"
      },
      "source": [
        "Let's take a peek and see what that looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOpydvc8eqNM",
        "outputId": "f924b59d-eb6b-4c1a-9d18-f545c4e2c724"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'Why does the author believe that intelligence is overrated?',\n",
              " 'answer': \"The author believes that intelligence is overrated because they think that hiring the smartest people does not guarantee a company's success.\",\n",
              " 'contexts': ['Especially in this industry.\\nYou will read, hire the smartest people out there and your com-\\npany’s success is all but guaranteed.\\nI think intelligence, per se, is highly overrated.',\n",
              "  'theory.\\nThe paper then digs into possible correlations between intelli-\\ngence as measured by such metrics as IQ, and creative output:',\n",
              "  '[E]ven if a minimal level of intelligence is requisite for achieve-\\nment, beyond a threshold of around IQ 120 (the actual amount\\nvarying across Xelds), intellectual prowess becomes largely irrele-',\n",
              "  'Now, clearly you don’t want to hire dumb people, and clearly\\nyou’d like to work with smart people.\\nBut let’s get speciXc.\\nMost of the lore in our industry about the role of intelligence'],\n",
              " 'ground_truth': 'The author believes that intelligence is overrated because they are unaware of any actual data that shows a correlation between intelligence and company success.'}"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM4fmAnsBmL2"
      },
      "source": [
        "# 🤝 Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbsFm5FievJI"
      },
      "source": [
        "## Task 1: Evaluating our Pipeline with Ragas\n",
        "\n",
        "Now that we have our response dataset - we can finally get into the \"meat\" of Ragas - evaluation!\n",
        "\n",
        "First, we'll import the desired metrics, then we can use them to evaluate our created dataset!\n",
        "\n",
        "Check out the specific metrics we'll be using in the Ragas documentation:\n",
        "\n",
        "- [Faithfulness](https://docs.ragas.io/en/stable/concepts/metrics/faithfulness.html)\n",
        "- [Answer Relevancy](https://docs.ragas.io/en/stable/concepts/metrics/answer_relevance.html)\n",
        "- [Context Precision](https://docs.ragas.io/en/stable/concepts/metrics/context_precision.html)\n",
        "- [Context Recall](https://docs.ragas.io/en/stable/concepts/metrics/context_recall.html)\n",
        "- [Answer Correctness](https://docs.ragas.io/en/stable/concepts/metrics/answer_correctness.html)\n",
        "\n",
        "See the accompanied presentation for more in-depth explanations about each of the metrics!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "R2PXwyt8e5aW"
      },
      "outputs": [],
      "source": [
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    answer_correctness,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        ")\n",
        "\n",
        "metrics = [\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_recall,\n",
        "    context_precision,\n",
        "    answer_correctness,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx-vlsx_hrtV"
      },
      "source": [
        "All that's left to do is call \"evaluate\" and away we go!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "32514310070a426ea247c9f1bc66b630",
            "3e7520df71de40e0af5589b6aeb95171",
            "05390d20f1b445b5b02529ee7a99f6d6",
            "3380693903474d2585638f7e3458fcd6",
            "1d43002974f24e8a8b6961cddc04ce47",
            "97abe811c89c44dcacd7e39074d22546",
            "87a3d4b2ed5f4f1ca895c6a1981eb847",
            "589c2004f5504a239615dec8671785d0",
            "25d3337c457f4c748ed8bf78f5a27fe8",
            "31064d2adec14238a609d3f9791c64f3",
            "4f482b8ce7a54c1787394fb7d90391a0"
          ]
        },
        "id": "DhlcfJ4lgYVI",
        "outputId": "85fa2a99-7506-45d1-a674-ca9f55372264"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c2e23c46a474982ae6498540821aaf3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No statements were generated from the answer.\n"
          ]
        }
      ],
      "source": [
        "results = evaluate(response_dataset, metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqPArpSrgwDD",
        "outputId": "e8f3cb2f-8a38-47a5-f54d-ec80eaca8448"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'faithfulness': 0.7481, 'answer_relevancy': 0.8522, 'context_recall': 0.5175, 'context_precision': 0.7690, 'answer_correctness': 0.4319}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2nsGzj8DhP9E",
        "outputId": "a10d6394-0ab7-48bf-96c5-acfc5992622f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Why does the author believe that intelligence ...</td>\n",
              "      <td>The author believes that intelligence is overr...</td>\n",
              "      <td>[Especially in this industry.\\nYou will read, ...</td>\n",
              "      <td>The author believes that intelligence is overr...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.613280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How important is it for a startup to focus on ...</td>\n",
              "      <td>It is important for a startup to focus on deve...</td>\n",
              "      <td>[competitor, be sure to take a step back and s...</td>\n",
              "      <td>Focusing on developing a large market is impor...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.938918</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.747455</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What impact does the presence of a billion peo...</td>\n",
              "      <td>The presence of a billion people online is cau...</td>\n",
              "      <td>[billion people online now. That is a very lar...</td>\n",
              "      <td>The presence of a billion people online has le...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.886831</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.572266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What are the consequences of not raising enoug...</td>\n",
              "      <td>Not raising enough money risks the survival of...</td>\n",
              "      <td>[Here’s why you shouldn’t do that:\\nWhat are t...</td>\n",
              "      <td>Not raising enough money risks the survival of...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.963161</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.533194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does a deal with a big company typically i...</td>\n",
              "      <td>A deal with the right big company can have a h...</td>\n",
              "      <td>[There are times in the life of a startup when...</td>\n",
              "      <td>A deal with a big company typically does not g...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.966861</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.234217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What industry is Washington DC known for in te...</td>\n",
              "      <td>Politics.</td>\n",
              "      <td>[the city where all the action is happening.\\n...</td>\n",
              "      <td>Washington DC is known for the politics industry.</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.855998</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.960259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What are some common layers of risk for a high...</td>\n",
              "      <td>Founder risk, technology risk, and product ris...</td>\n",
              "      <td>[What are the layers of risk for a high-tech\\n...</td>\n",
              "      <td>Here are some common layers of risk for a high...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.993852</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.538804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How are high expectations beneficial in a high...</td>\n",
              "      <td>High expectations are beneficial in a high-gro...</td>\n",
              "      <td>[Third rule:\\nIn a rapidly changing Held like ...</td>\n",
              "      <td>High expectations in a high-growth company can...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.992456</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.575004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the quality of output in a creative c...</td>\n",
              "      <td>The quality of output in a creative career rel...</td>\n",
              "      <td>[creator’s most distinguished work will appear...</td>\n",
              "      <td>Quality of output in a creative career does no...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.225367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What are the criteria for evaluating candidate...</td>\n",
              "      <td>Criteria for evaluating candidates when hiring...</td>\n",
              "      <td>[How to hire the best people you've\\never work...</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.183508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How does perceived risk impact decision-making...</td>\n",
              "      <td>I don't know.</td>\n",
              "      <td>[What are the layers of risk for a high-tech\\n...</td>\n",
              "      <td>The desire to be liked in a startup with multi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.181114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What is the purpose of the \"Pending\" email fol...</td>\n",
              "      <td>The purpose of the \"Pending\" email folder is t...</td>\n",
              "      <td>[the normal course of your day.\\nFourth, aside...</td>\n",
              "      <td>The purpose of the \"Pending\" email folder is t...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.932537</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.410318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>What advantages does a technical degree offer ...</td>\n",
              "      <td>A technical degree offers the advantage of dev...</td>\n",
              "      <td>[path to doing anything big.\\n•\\nPlus, technic...</td>\n",
              "      <td>Technical degrees offer the advantage of teach...</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.925023</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.536647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>What is the correlation between startup team c...</td>\n",
              "      <td>The correlation between startup team caliber a...</td>\n",
              "      <td>[Let’s start by deXning terms.\\nThe caliber of...</td>\n",
              "      <td>nan</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.903619</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>What factors should be considered when making ...</td>\n",
              "      <td>Factors that should be considered when making ...</td>\n",
              "      <td>[order to execute its plan? E.g. a startup pla...</td>\n",
              "      <td>The factors that should be considered when mak...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.958568</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.677179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>What forms of chance exist and how do they rel...</td>\n",
              "      <td>The forms of chance that exist are Chance III ...</td>\n",
              "      <td>[We can observe chance arriving in four major ...</td>\n",
              "      <td>The four forms of chance are Chance I and Chan...</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.904614</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.805556</td>\n",
              "      <td>0.374413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>How does the Liking/Loving Tendency affect ent...</td>\n",
              "      <td>The Liking/Loving Tendency affects entrepreneu...</td>\n",
              "      <td>[other facts to facilitate love.\\nThe applicat...</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.181389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>What is the purpose of temporary subfolders in...</td>\n",
              "      <td>The purpose of temporary subfolders in email o...</td>\n",
              "      <td>[matter, right now.\\nThose subfolders then get...</td>\n",
              "      <td>The purpose of temporary subfolders in email o...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.969234</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.246163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>What is the impact of the shift towards restri...</td>\n",
              "      <td>The impact of the shift towards restricted sto...</td>\n",
              "      <td>[a more appropriate motivator of employees of ...</td>\n",
              "      <td>The shift towards restricted stock and away fr...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.229331</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             question  \\\n",
              "0   Why does the author believe that intelligence ...   \n",
              "1   How important is it for a startup to focus on ...   \n",
              "2   What impact does the presence of a billion peo...   \n",
              "3   What are the consequences of not raising enoug...   \n",
              "4   How does a deal with a big company typically i...   \n",
              "5   What industry is Washington DC known for in te...   \n",
              "6   What are some common layers of risk for a high...   \n",
              "7   How are high expectations beneficial in a high...   \n",
              "8   How does the quality of output in a creative c...   \n",
              "9   What are the criteria for evaluating candidate...   \n",
              "10  How does perceived risk impact decision-making...   \n",
              "11  What is the purpose of the \"Pending\" email fol...   \n",
              "12  What advantages does a technical degree offer ...   \n",
              "13  What is the correlation between startup team c...   \n",
              "14  What factors should be considered when making ...   \n",
              "15  What forms of chance exist and how do they rel...   \n",
              "16  How does the Liking/Loving Tendency affect ent...   \n",
              "17  What is the purpose of temporary subfolders in...   \n",
              "18  What is the impact of the shift towards restri...   \n",
              "\n",
              "                                               answer  \\\n",
              "0   The author believes that intelligence is overr...   \n",
              "1   It is important for a startup to focus on deve...   \n",
              "2   The presence of a billion people online is cau...   \n",
              "3   Not raising enough money risks the survival of...   \n",
              "4   A deal with the right big company can have a h...   \n",
              "5                                           Politics.   \n",
              "6   Founder risk, technology risk, and product ris...   \n",
              "7   High expectations are beneficial in a high-gro...   \n",
              "8   The quality of output in a creative career rel...   \n",
              "9   Criteria for evaluating candidates when hiring...   \n",
              "10                                      I don't know.   \n",
              "11  The purpose of the \"Pending\" email folder is t...   \n",
              "12  A technical degree offers the advantage of dev...   \n",
              "13  The correlation between startup team caliber a...   \n",
              "14  Factors that should be considered when making ...   \n",
              "15  The forms of chance that exist are Chance III ...   \n",
              "16  The Liking/Loving Tendency affects entrepreneu...   \n",
              "17  The purpose of temporary subfolders in email o...   \n",
              "18  The impact of the shift towards restricted sto...   \n",
              "\n",
              "                                             contexts  \\\n",
              "0   [Especially in this industry.\\nYou will read, ...   \n",
              "1   [competitor, be sure to take a step back and s...   \n",
              "2   [billion people online now. That is a very lar...   \n",
              "3   [Here’s why you shouldn’t do that:\\nWhat are t...   \n",
              "4   [There are times in the life of a startup when...   \n",
              "5   [the city where all the action is happening.\\n...   \n",
              "6   [What are the layers of risk for a high-tech\\n...   \n",
              "7   [Third rule:\\nIn a rapidly changing Held like ...   \n",
              "8   [creator’s most distinguished work will appear...   \n",
              "9   [How to hire the best people you've\\never work...   \n",
              "10  [What are the layers of risk for a high-tech\\n...   \n",
              "11  [the normal course of your day.\\nFourth, aside...   \n",
              "12  [path to doing anything big.\\n•\\nPlus, technic...   \n",
              "13  [Let’s start by deXning terms.\\nThe caliber of...   \n",
              "14  [order to execute its plan? E.g. a startup pla...   \n",
              "15  [We can observe chance arriving in four major ...   \n",
              "16  [other facts to facilitate love.\\nThe applicat...   \n",
              "17  [matter, right now.\\nThose subfolders then get...   \n",
              "18  [a more appropriate motivator of employees of ...   \n",
              "\n",
              "                                         ground_truth  faithfulness  \\\n",
              "0   The author believes that intelligence is overr...      0.500000   \n",
              "1   Focusing on developing a large market is impor...      0.000000   \n",
              "2   The presence of a billion people online has le...      1.000000   \n",
              "3   Not raising enough money risks the survival of...      1.000000   \n",
              "4   A deal with a big company typically does not g...      1.000000   \n",
              "5   Washington DC is known for the politics industry.      1.000000   \n",
              "6   Here are some common layers of risk for a high...      1.000000   \n",
              "7   High expectations in a high-growth company can...      0.000000   \n",
              "8   Quality of output in a creative career does no...      1.000000   \n",
              "9                                                 nan      0.500000   \n",
              "10  The desire to be liked in a startup with multi...           NaN   \n",
              "11  The purpose of the \"Pending\" email folder is t...      1.000000   \n",
              "12  Technical degrees offer the advantage of teach...      0.500000   \n",
              "13                                                nan      1.000000   \n",
              "14  The factors that should be considered when mak...      0.666667   \n",
              "15  The four forms of chance are Chance I and Chan...      0.800000   \n",
              "16                                                nan      0.500000   \n",
              "17  The purpose of temporary subfolders in email o...      1.000000   \n",
              "18  The shift towards restricted stock and away fr...      1.000000   \n",
              "\n",
              "    answer_relevancy  context_recall  context_precision  answer_correctness  \n",
              "0           1.000000        1.000000           0.416667            0.613280  \n",
              "1           0.938918        1.000000           1.000000            0.747455  \n",
              "2           0.886831        1.000000           1.000000            0.572266  \n",
              "3           0.963161        0.500000           0.805556            0.533194  \n",
              "4           0.966861        1.000000           1.000000            0.234217  \n",
              "5           0.855998        1.000000           1.000000            0.960259  \n",
              "6           0.993852        0.000000           1.000000            0.538804  \n",
              "7           0.992456        0.000000           1.000000            0.575004  \n",
              "8           1.000000        0.000000           1.000000            0.225367  \n",
              "9           1.000000        0.000000           0.000000            0.183508  \n",
              "10          0.000000        0.000000           0.916667            0.181114  \n",
              "11          0.932537        1.000000           0.916667            0.410318  \n",
              "12          0.925023        1.000000           1.000000            0.536647  \n",
              "13          0.903619        0.000000           0.000000            0.187036  \n",
              "14          0.958568        0.000000           1.000000            0.677179  \n",
              "15          0.904614        0.333333           0.805556            0.374413  \n",
              "16          1.000000        0.666667           0.000000            0.181389  \n",
              "17          0.969234        1.000000           1.000000            0.246163  \n",
              "18          0.000000        0.333333           0.750000            0.229331  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df = results.to_pandas()\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWfiu_pLh3JL"
      },
      "source": [
        "## Task 2: Making Adjustments to our RAG Pipeline\n",
        "\n",
        "Now that we have established a baseline - we can see how any changes impact our pipeline's performance!\n",
        "\n",
        "Let's modify our retriever and see how that impacts our Ragas metrics!\n",
        "\n",
        "> NOTE: MultiQueryRetriever is expanded on [here](https://python.langchain.com/docs/modules/data_connection/retrievers/MultiQueryRetriever) but for now, the implementation is not important to our lesson!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "nKIuM336isBL"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import MultiQueryRetriever\n",
        "\n",
        "advanced_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=primary_qa_llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82rcj3L-i_c8"
      },
      "source": [
        "We'll also re-create our RAG pipeline using the abstractions that come packaged with LangChain v0.1.0!\n",
        "\n",
        "First, let's create a chain to \"stuff\" our documents into our context!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "EfdCgTw7jC4i"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "document_chain = create_stuff_documents_chain(primary_qa_llm, retrieval_qa_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozYl5WdPnvLu"
      },
      "source": [
        "Next, we'll create the retrieval chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9AK7wHVnn0U3"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "retrieval_chain = create_retrieval_chain(advanced_retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "cmKORMfMoCjL"
      },
      "outputs": [],
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"Who is Taylor Swift fueding with?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICMsUWbWoOpf",
        "outputId": "1c2a8c65-0da8-44ef-d6e4-79dcca738777"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I'm sorry, but based on the context provided, I do not have information about any feud involving Taylor Swift.\n"
          ]
        }
      ],
      "source": [
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "5s8ZGasYoVi6"
      },
      "outputs": [],
      "source": [
        "response = retrieval_chain.invoke({\"input\": \"Why are they fueding?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADNCdW4hoYT8",
        "outputId": "40860a4e-75fb-486e-b1e6-34a7eb2c5295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There is no information provided in the context about any feud or conflict between individuals or groups.\n"
          ]
        }
      ],
      "source": [
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxkU0HdpoaiE"
      },
      "source": [
        "Well, just from those responses this chain *feels* better - but lets see how it performs on our eval!\n",
        "\n",
        "Let's do the same process we did before to collect our pipeline's contexts and answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "kO8cWxn2oinT"
      },
      "outputs": [],
      "source": [
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for question in test_questions:\n",
        "  response = retrieval_chain.invoke({\"input\" : question})\n",
        "  answers.append(response[\"answer\"])\n",
        "  contexts.append([context.page_content for context in response[\"context\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgagfhPUtM2j"
      },
      "source": [
        "Now we can convert this into a dataset, just like we did before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "5FcllGeSovP8"
      },
      "outputs": [],
      "source": [
        "response_dataset_advanced_retrieval = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : answers,\n",
        "    \"contexts\" : contexts,\n",
        "    \"ground_truth\" : test_groundtruths\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dELYabwktR2C"
      },
      "source": [
        "Let's evaluate on the same metrics we did for the first pipeline and see how it does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "831b4dab6ff94d239d2824d390e01308",
            "4cefefc6cf714a68924e1b8d5e59aba9",
            "fd7f5542a22d44388dda12ca19443a1f",
            "93bf9b194c04460abffa192c19bcf67b",
            "83985f58744a46cfbd001ce5957f3e4a",
            "5c2b92989d7448e9bf65306c4f2f7d93",
            "a34b3906cd514234a115c7bf6757ca9d",
            "a03fefb9fa5a40ff947dc4ccd3c80318",
            "40343486e3ea4e5fae55b5a528f139d8",
            "cee2268aa21643e6ad77117c67ec1600",
            "c79f28da88f44d69aa87905c089df333"
          ]
        },
        "id": "d7uHseWJo2TU",
        "outputId": "8facfba9-467a-4129-b381-ddbd0a952f28"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00a017997fab45b589853e48626da119",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-7sNMKGgrfbpAaV9PNLyBjIx1 on tokens per min (TPM): Limit 60000, Used 59010, Requested 1721. Please try again in 731ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-7sNMKGgrfbpAaV9PNLyBjIx1 on tokens per min (TPM): Limit 60000, Used 59349, Requested 1466. Please try again in 815ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/_answer_correctness.py\", line 250, in _ascore\n",
            "    is_statement_present = await self.llm.generate(\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-7sNMKGgrfbpAaV9PNLyBjIx1 on tokens per min (TPM): Limit 60000, Used 58918, Requested 1409. Please try again in 327ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-7sNMKGgrfbpAaV9PNLyBjIx1 on tokens per min (TPM): Limit 60000, Used 59465, Requested 1462. Please try again in 927ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
          ]
        }
      ],
      "source": [
        "advanced_retrieval_results = evaluate(response_dataset_advanced_retrieval, metrics, raise_exceptions=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JsFd0uDd2n5E",
        "outputId": "0922ab45-c16a-48c4-c9c2-3647ff130391"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>contexts</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_recall</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>answer_correctness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Why does the author believe that intelligence ...</td>\n",
              "      <td>The author believes that intelligence is overr...</td>\n",
              "      <td>[Especially in this industry.\\nYou will read, ...</td>\n",
              "      <td>The author believes that intelligence is overr...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How important is it for a startup to focus on ...</td>\n",
              "      <td>It is very important for a startup to focus on...</td>\n",
              "      <td>[answer, in part because in the beginning of a...</td>\n",
              "      <td>Focusing on developing a large market is impor...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.200000</td>\n",
              "      <td>0.528415</td>\n",
              "      <td>0.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What impact does the presence of a billion peo...</td>\n",
              "      <td>The presence of a billion people online has le...</td>\n",
              "      <td>[Which makes total sense, amid the enormous ma...</td>\n",
              "      <td>The presence of a billion people online has le...</td>\n",
              "      <td>0.946296</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.564995</td>\n",
              "      <td>0.857143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What are the consequences of not raising enoug...</td>\n",
              "      <td>Not raising enough money risks the survival of...</td>\n",
              "      <td>[Here’s why you shouldn’t do that:\\nWhat are t...</td>\n",
              "      <td>Not raising enough money risks the survival of...</td>\n",
              "      <td>0.989127</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.963161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How does a deal with a big company typically i...</td>\n",
              "      <td>A deal with a big company typically does not l...</td>\n",
              "      <td>[There are times in the life of a startup when...</td>\n",
              "      <td>A deal with a big company typically does not g...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.810000</td>\n",
              "      <td>0.447298</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.940896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>What industry is Washington DC known for in te...</td>\n",
              "      <td>Washington DC is known for the politics industry.</td>\n",
              "      <td>[the city where all the action is happening.\\n...</td>\n",
              "      <td>Washington DC is known for the politics industry.</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.638889</td>\n",
              "      <td>0.243393</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.925706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What are some common layers of risk for a high...</td>\n",
              "      <td>Some common layers of risk for a high-tech sta...</td>\n",
              "      <td>[What are the layers of risk for a high-tech\\n...</td>\n",
              "      <td>Here are some common layers of risk for a high...</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>How are high expectations beneficial in a high...</td>\n",
              "      <td>High expectations in a high-growth company can...</td>\n",
              "      <td>[Third rule:\\nIn a rapidly changing Held like ...</td>\n",
              "      <td>High expectations in a high-growth company can...</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.494258</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.978908</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How does the quality of output in a creative c...</td>\n",
              "      <td>The relation between quantity and quality of o...</td>\n",
              "      <td>[creator’s most distinguished work will appear...</td>\n",
              "      <td>Quality of output in a creative career does no...</td>\n",
              "      <td>0.892857</td>\n",
              "      <td>0.779570</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.949753</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What are the criteria for evaluating candidate...</td>\n",
              "      <td>When hiring the best people, it is important t...</td>\n",
              "      <td>[with your team as you interview candidates fo...</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.950000</td>\n",
              "      <td>0.223059</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.950783</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>How does perceived risk impact decision-making...</td>\n",
              "      <td>Perceived risk can impact decision-making in a...</td>\n",
              "      <td>[What are the layers of risk for a high-tech\\n...</td>\n",
              "      <td>The desire to be liked in a startup with multi...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.179971</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.906517</td>\n",
              "      <td>0.739342</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What is the purpose of the \"Pending\" email fol...</td>\n",
              "      <td>The \"Pending\" email folder is used for emails ...</td>\n",
              "      <td>[the normal course of your day.\\nFourth, aside...</td>\n",
              "      <td>The purpose of the \"Pending\" email folder is t...</td>\n",
              "      <td>0.452308</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.929662</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.633333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>What advantages does a technical degree offer ...</td>\n",
              "      <td>A technical degree offers advantages in terms ...</td>\n",
              "      <td>[path to doing anything big.\\n•\\nPlus, technic...</td>\n",
              "      <td>Technical degrees offer the advantage of teach...</td>\n",
              "      <td>0.618381</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.976354</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>What is the correlation between startup team c...</td>\n",
              "      <td>The caliber of a startup team can greatly impa...</td>\n",
              "      <td>[Let’s start by deXning terms.\\nThe caliber of...</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.450368</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.882780</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>What factors should be considered when making ...</td>\n",
              "      <td>When making executive hires in startups, it is...</td>\n",
              "      <td>[order to execute its plan? E.g. a startup pla...</td>\n",
              "      <td>The factors that should be considered when mak...</td>\n",
              "      <td>0.185224</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.935965</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>What forms of chance exist and how do they rel...</td>\n",
              "      <td>There are four forms of chance mentioned in th...</td>\n",
              "      <td>[We can observe chance arriving in four major ...</td>\n",
              "      <td>The four forms of chance are Chance I and Chan...</td>\n",
              "      <td>0.565687</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.862481</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>How does the Liking/Loving Tendency affect ent...</td>\n",
              "      <td>The Liking/Loving Tendency can lead entreprene...</td>\n",
              "      <td>[other facts to facilitate love.\\nThe applicat...</td>\n",
              "      <td>nan</td>\n",
              "      <td>0.538422</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.919354</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>What is the purpose of temporary subfolders in...</td>\n",
              "      <td>Temporary subfolders in email organization are...</td>\n",
              "      <td>[matter, right now.\\nThose subfolders then get...</td>\n",
              "      <td>The purpose of temporary subfolders in email o...</td>\n",
              "      <td>0.182486</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.959444</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>What is the impact of the shift towards restri...</td>\n",
              "      <td>The shift towards restricted stock and away fr...</td>\n",
              "      <td>[a more appropriate motivator of employees of ...</td>\n",
              "      <td>The shift towards restricted stock and away fr...</td>\n",
              "      <td>0.671120</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.936771</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.714932</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             question  \\\n",
              "0   Why does the author believe that intelligence ...   \n",
              "1   How important is it for a startup to focus on ...   \n",
              "2   What impact does the presence of a billion peo...   \n",
              "3   What are the consequences of not raising enoug...   \n",
              "4   How does a deal with a big company typically i...   \n",
              "5   What industry is Washington DC known for in te...   \n",
              "6   What are some common layers of risk for a high...   \n",
              "7   How are high expectations beneficial in a high...   \n",
              "8   How does the quality of output in a creative c...   \n",
              "9   What are the criteria for evaluating candidate...   \n",
              "10  How does perceived risk impact decision-making...   \n",
              "11  What is the purpose of the \"Pending\" email fol...   \n",
              "12  What advantages does a technical degree offer ...   \n",
              "13  What is the correlation between startup team c...   \n",
              "14  What factors should be considered when making ...   \n",
              "15  What forms of chance exist and how do they rel...   \n",
              "16  How does the Liking/Loving Tendency affect ent...   \n",
              "17  What is the purpose of temporary subfolders in...   \n",
              "18  What is the impact of the shift towards restri...   \n",
              "\n",
              "                                               answer  \\\n",
              "0   The author believes that intelligence is overr...   \n",
              "1   It is very important for a startup to focus on...   \n",
              "2   The presence of a billion people online has le...   \n",
              "3   Not raising enough money risks the survival of...   \n",
              "4   A deal with a big company typically does not l...   \n",
              "5   Washington DC is known for the politics industry.   \n",
              "6   Some common layers of risk for a high-tech sta...   \n",
              "7   High expectations in a high-growth company can...   \n",
              "8   The relation between quantity and quality of o...   \n",
              "9   When hiring the best people, it is important t...   \n",
              "10  Perceived risk can impact decision-making in a...   \n",
              "11  The \"Pending\" email folder is used for emails ...   \n",
              "12  A technical degree offers advantages in terms ...   \n",
              "13  The caliber of a startup team can greatly impa...   \n",
              "14  When making executive hires in startups, it is...   \n",
              "15  There are four forms of chance mentioned in th...   \n",
              "16  The Liking/Loving Tendency can lead entreprene...   \n",
              "17  Temporary subfolders in email organization are...   \n",
              "18  The shift towards restricted stock and away fr...   \n",
              "\n",
              "                                             contexts  \\\n",
              "0   [Especially in this industry.\\nYou will read, ...   \n",
              "1   [answer, in part because in the beginning of a...   \n",
              "2   [Which makes total sense, amid the enormous ma...   \n",
              "3   [Here’s why you shouldn’t do that:\\nWhat are t...   \n",
              "4   [There are times in the life of a startup when...   \n",
              "5   [the city where all the action is happening.\\n...   \n",
              "6   [What are the layers of risk for a high-tech\\n...   \n",
              "7   [Third rule:\\nIn a rapidly changing Held like ...   \n",
              "8   [creator’s most distinguished work will appear...   \n",
              "9   [with your team as you interview candidates fo...   \n",
              "10  [What are the layers of risk for a high-tech\\n...   \n",
              "11  [the normal course of your day.\\nFourth, aside...   \n",
              "12  [path to doing anything big.\\n•\\nPlus, technic...   \n",
              "13  [Let’s start by deXning terms.\\nThe caliber of...   \n",
              "14  [order to execute its plan? E.g. a startup pla...   \n",
              "15  [We can observe chance arriving in four major ...   \n",
              "16  [other facts to facilitate love.\\nThe applicat...   \n",
              "17  [matter, right now.\\nThose subfolders then get...   \n",
              "18  [a more appropriate motivator of employees of ...   \n",
              "\n",
              "                                         ground_truth  faithfulness  \\\n",
              "0   The author believes that intelligence is overr...           NaN   \n",
              "1   Focusing on developing a large market is impor...      1.000000   \n",
              "2   The presence of a billion people online has le...      0.946296   \n",
              "3   Not raising enough money risks the survival of...      0.989127   \n",
              "4   A deal with a big company typically does not g...      1.000000   \n",
              "5   Washington DC is known for the politics industry.      1.000000   \n",
              "6   Here are some common layers of risk for a high...      0.666667   \n",
              "7   High expectations in a high-growth company can...      0.950000   \n",
              "8   Quality of output in a creative career does no...      0.892857   \n",
              "9                                                 nan      0.950000   \n",
              "10  The desire to be liked in a startup with multi...      0.000000   \n",
              "11  The purpose of the \"Pending\" email folder is t...      0.452308   \n",
              "12  Technical degrees offer the advantage of teach...      0.618381   \n",
              "13                                                nan      0.450368   \n",
              "14  The factors that should be considered when mak...      0.185224   \n",
              "15  The four forms of chance are Chance I and Chan...      0.565687   \n",
              "16                                                nan      0.538422   \n",
              "17  The purpose of temporary subfolders in email o...      0.182486   \n",
              "18  The shift towards restricted stock and away fr...      0.671120   \n",
              "\n",
              "    answer_relevancy  context_recall  context_precision  answer_correctness  \n",
              "0                NaN             NaN                NaN            1.000000  \n",
              "1           1.000000        0.200000           0.528415            0.600000  \n",
              "2           1.000000        1.000000           0.564995            0.857143  \n",
              "3           0.500000        1.000000           1.000000            0.963161  \n",
              "4           0.810000        0.447298           1.000000            0.940896  \n",
              "5           0.638889        0.243393           1.000000            0.925706  \n",
              "6           1.000000        1.000000           1.000000            0.000000  \n",
              "7           0.494258        0.000000           0.978908            1.000000  \n",
              "8           0.779570        0.666667           0.949753            0.000000  \n",
              "9           0.223059        1.000000           0.950783            1.000000  \n",
              "10          0.179971        1.000000           0.906517            0.739342  \n",
              "11          1.000000        0.929662           1.000000            0.633333  \n",
              "12          0.833333        0.976354           1.000000            1.000000  \n",
              "13          0.750000        0.882780           0.166667            0.000000  \n",
              "14          1.000000        0.935965           0.000000            1.000000  \n",
              "15          0.750000        0.862481           0.333333            1.000000  \n",
              "16          1.000000        0.919354           0.428571            0.000000  \n",
              "17          1.000000        0.959444           1.000000            0.833333  \n",
              "18          0.750000        0.936771           0.700000            0.714932  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "advanced_retrieval_results_df = advanced_retrieval_results.to_pandas()\n",
        "advanced_retrieval_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0hzqq5VtZ2a"
      },
      "source": [
        "## Task 3: Evaluating our Adjusted Pipeline Against Our Baseline\n",
        "\n",
        "Now we can compare our results and see what directional changes occured!\n",
        "\n",
        "Let's refresh with our initial metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WWGRaF5qx3V",
        "outputId": "7924b9a5-1bfc-4d26-f1f7-75e10fb64857"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'faithfulness': 0.7481, 'answer_relevancy': 0.8522, 'context_recall': 0.5175, 'context_precision': 0.7690, 'answer_correctness': 0.4319}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFv_yAeotmFs"
      },
      "source": [
        "And see how our advanced retrieval modified our chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpV11dxJo7xa",
        "outputId": "2ce6e4b7-f037-4fd4-ca0c-1d47ff5dc522"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'faithfulness': 0.6699, 'answer_relevancy': 0.7616, 'context_recall': 0.7756, 'context_precision': 0.7504, 'answer_correctness': 0.6951}"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "advanced_retrieval_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "62NYn3iAvTjM",
        "outputId": "732eec56-d4ef-4403-cc90-62d0c81c37cf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Baseline</th>\n",
              "      <th>MultiQueryRetriever with Document Stuffing</th>\n",
              "      <th>Delta</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>faithfulness</td>\n",
              "      <td>0.748148</td>\n",
              "      <td>0.669941</td>\n",
              "      <td>-0.078207</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>answer_relevancy</td>\n",
              "      <td>0.852193</td>\n",
              "      <td>0.761616</td>\n",
              "      <td>-0.090578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>context_recall</td>\n",
              "      <td>0.517544</td>\n",
              "      <td>0.775565</td>\n",
              "      <td>0.258021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>context_precision</td>\n",
              "      <td>0.769006</td>\n",
              "      <td>0.750441</td>\n",
              "      <td>-0.018565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>answer_correctness</td>\n",
              "      <td>0.431944</td>\n",
              "      <td>0.695150</td>\n",
              "      <td>0.263205</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Metric  Baseline  MultiQueryRetriever with Document Stuffing  \\\n",
              "0        faithfulness  0.748148                                    0.669941   \n",
              "1    answer_relevancy  0.852193                                    0.761616   \n",
              "2      context_recall  0.517544                                    0.775565   \n",
              "3   context_precision  0.769006                                    0.750441   \n",
              "4  answer_correctness  0.431944                                    0.695150   \n",
              "\n",
              "      Delta  \n",
              "0 -0.078207  \n",
              "1 -0.090578  \n",
              "2  0.258021  \n",
              "3 -0.018565  \n",
              "4  0.263205  "
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_original = pd.DataFrame(list(results.items()), columns=['Metric', 'Baseline'])\n",
        "df_comparison = pd.DataFrame(list(advanced_retrieval_results.items()), columns=['Metric', 'MultiQueryRetriever with Document Stuffing'])\n",
        "\n",
        "df_merged = pd.merge(df_original, df_comparison, on='Metric')\n",
        "\n",
        "df_merged['Delta'] = df_merged['MultiQueryRetriever with Document Stuffing'] - df_merged['Baseline']\n",
        "\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJKEOLNs5v0R"
      },
      "source": [
        "## Task 4: Testing OpenAI's Claim\n",
        "\n",
        "Now that we've seen how our retriever can impact the performance of our RAG pipeline - let's see how changing our embedding model impacts performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM4KRhJYEL-h"
      },
      "source": [
        "####🏗️ Activity #2:\n",
        "\n",
        "Please provide markdown, or code comments, to explain which each of the following steps are doing!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Gv_tv4w86bPb"
      },
      "outputs": [],
      "source": [
        "# creating embedding model object instance\n",
        "\n",
        "new_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "-JPe1_Jx6Rnw"
      },
      "outputs": [],
      "source": [
        "# creating qdrant vector store stored in memory\n",
        "vector_store = Qdrant.from_documents(\n",
        "    documents,\n",
        "    new_embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"PMarca Blogs - TE3 - MQR\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "H-HuozNf6muZ"
      },
      "outputs": [],
      "source": [
        "# create retriver\n",
        "\n",
        "new_retriever = vector_store.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "M6Tyc3ZY7Km2"
      },
      "outputs": [],
      "source": [
        "# advanced retriver using multi query retriver\n",
        "\n",
        "new_advanced_retriever = MultiQueryRetriever.from_llm(retriever=new_retriever, llm=primary_qa_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "s5QSJIhm7SKr"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "new_retrieval_chain = create_retrieval_chain(new_advanced_retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "MBVjl1UK7fd7"
      },
      "outputs": [],
      "source": [
        "# prepare data for evaluation dataset using new retrival chan\n",
        "\n",
        "answers = []\n",
        "contexts = []\n",
        "\n",
        "for question in test_questions:\n",
        "  response = new_retrieval_chain.invoke({\"input\" : question})\n",
        "  answers.append(response[\"answer\"])\n",
        "  contexts.append([context.page_content for context in response[\"context\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "lTBrs0zr7iyG"
      },
      "outputs": [],
      "source": [
        "# transform our lists in evaluation dataset\n",
        "\n",
        "new_response_dataset_advanced_retrieval = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : answers,\n",
        "    \"contexts\" : contexts,\n",
        "    \"ground_truth\" : test_groundtruths\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "399f6ec046c34c26818a07c5efc6845a",
            "7f82e0a3c3684460b8dda773d283b535",
            "cfa01f60b62f4a88806d85cee5ac0fa6",
            "38988d3f6f5f4de3b3d4be7fec89c3c7",
            "87bd0ad74d4345dea4b409d64524f6e7",
            "8cb506949697432db061878397d196f1",
            "e7831a581d024e3ebb4026a89ceef127",
            "18701fc64eb44d26b8aa1ae0af64d09f",
            "a6581091161c489d877c2cfec432f6ae",
            "42dcc945d1624f69b63bed2fa52cc4fa",
            "5524289f1e594a5eac60ee29d9f4249c"
          ]
        },
        "id": "hG5h-D8n7sZp",
        "outputId": "6760b49b-4576-4bee-e61a-293df24e2bc3"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2478524feb894994a8cece74b4b4e2bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/95 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-7sNMKGgrfbpAaV9PNLyBjIx1 on tokens per min (TPM): Limit 60000, Used 59154, Requested 1464. Please try again in 618ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/_context_precision.py\", line 161, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-7sNMKGgrfbpAaV9PNLyBjIx1 on tokens per min (TPM): Limit 60000, Used 59027, Requested 1305. Please try again in 332ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/_answer_correctness.py\", line 250, in _ascore\n",
            "    is_statement_present = await self.llm.generate(\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-7sNMKGgrfbpAaV9PNLyBjIx1 on tokens per min (TPM): Limit 60000, Used 59581, Requested 1414. Please try again in 994ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-7sNMKGgrfbpAaV9PNLyBjIx1 on tokens per min (TPM): Limit 60000, Used 59061, Requested 1520. Please try again in 581ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-7sNMKGgrfbpAaV9PNLyBjIx1 on tokens per min (TPM): Limit 60000, Used 59152, Requested 1608. Please try again in 760ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
            "Runner in Executor raised an exception\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 78, in _aresults\n",
            "    r = await future\n",
            "        ^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/asyncio/tasks.py\", line 615, in _wait_for_one\n",
            "    return f.result()  # May raise f.exception().\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 37, in sema_coro\n",
            "    return await coro\n",
            "           ^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/executor.py\", line 111, in wrapped_callable_async\n",
            "    return counter, await callable(*args, **kwargs)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 125, in ascore\n",
            "    raise e\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/base.py\", line 121, in ascore\n",
            "    score = await self._ascore(row=row, callbacks=group_cm, is_async=is_async)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/metrics/_context_recall.py\", line 169, in _ascore\n",
            "    results = await self.llm.generate(\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 93, in generate\n",
            "    return await agenerate_text_with_retry(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 142, in async_wrapped\n",
            "    return await fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 58, in __call__\n",
            "    do = await self.iter(retry_state=retry_state)\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 110, in iter\n",
            "    result = await action(retry_state)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 78, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 410, in exc_check\n",
            "    raise retry_exc.reraise()\n",
            "          ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/__init__.py\", line 183, in reraise\n",
            "    raise self.last_attempt.result()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/tenacity/_asyncio.py\", line 61, in __call__\n",
            "    result = await fn(*args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/ragas/llms/base.py\", line 170, in agenerate_text\n",
            "    return await self.langchain_llm.agenerate_prompt(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 609, in agenerate_prompt\n",
            "    return await self.agenerate(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 569, in agenerate\n",
            "    raise exceptions[0]\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 754, in _agenerate_with_cache\n",
            "    result = await self._agenerate(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 657, in _agenerate\n",
            "    response = await self.async_client.create(messages=message_dicts, **params)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/resources/chat/completions.py\", line 1214, in create\n",
            "    return await self._post(\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1790, in post\n",
            "    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1493, in request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1569, in _request\n",
            "    return await self._retry_request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1615, in _retry_request\n",
            "    return await self._request(\n",
            "           ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/mpav/anaconda3/envs/llmops-w5/lib/python3.11/site-packages/openai/_base_client.py\", line 1584, in _request\n",
            "    raise self._make_status_error_from_response(err.response) from None\n",
            "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo in organization org-7sNMKGgrfbpAaV9PNLyBjIx1 on tokens per min (TPM): Limit 60000, Used 59620, Requested 1378. Please try again in 998ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n"
          ]
        }
      ],
      "source": [
        "# evaluate results of our new advanced retrival\n",
        "\n",
        "new_advanced_retrieval_results = evaluate(new_response_dataset_advanced_retrieval, metrics, raise_exceptions=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uHdcpsZ76kj",
        "outputId": "53a4f4ef-13ce-4a89-a06e-6c255ed3c025"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'faithfulness': 0.6380, 'answer_relevancy': 0.8383, 'context_recall': 0.6390, 'context_precision': 0.6832, 'answer_correctness': 0.7167}"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# result of our our evaluation\n",
        "# each score is the mean of individual scores of all the samples\n",
        "\n",
        "new_advanced_retrieval_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_items([('faithfulness', 0.6379657973587431), ('answer_relevancy', 0.8383408372712), ('context_recall', 0.6389906714815149), ('context_precision', 0.6832114840111884), ('answer_correctness', 0.7167423866934213)])"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_advanced_retrieval_results.items()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "s4TyaCUQ79Ke",
        "outputId": "c496266c-bf4a-4a3a-cd27-1d8abbf5bdb3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>ADA + Baseline</th>\n",
              "      <th>ADA + MQR</th>\n",
              "      <th>TE3 + MQR</th>\n",
              "      <th>ADA + MQR -&gt; TE3 + MQR</th>\n",
              "      <th>Baseline -&gt; TE3 + MQR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>faithfulness</td>\n",
              "      <td>0.748148</td>\n",
              "      <td>0.669941</td>\n",
              "      <td>0.637966</td>\n",
              "      <td>-0.031976</td>\n",
              "      <td>-0.110182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>answer_relevancy</td>\n",
              "      <td>0.852193</td>\n",
              "      <td>0.761616</td>\n",
              "      <td>0.838341</td>\n",
              "      <td>0.076725</td>\n",
              "      <td>-0.013852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>context_recall</td>\n",
              "      <td>0.517544</td>\n",
              "      <td>0.775565</td>\n",
              "      <td>0.638991</td>\n",
              "      <td>-0.136574</td>\n",
              "      <td>0.121447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>context_precision</td>\n",
              "      <td>0.769006</td>\n",
              "      <td>0.750441</td>\n",
              "      <td>0.683211</td>\n",
              "      <td>-0.067230</td>\n",
              "      <td>-0.085794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>answer_correctness</td>\n",
              "      <td>0.431944</td>\n",
              "      <td>0.695150</td>\n",
              "      <td>0.716742</td>\n",
              "      <td>0.021593</td>\n",
              "      <td>0.284798</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               Metric  ADA + Baseline  ADA + MQR  TE3 + MQR  \\\n",
              "0        faithfulness        0.748148   0.669941   0.637966   \n",
              "1    answer_relevancy        0.852193   0.761616   0.838341   \n",
              "2      context_recall        0.517544   0.775565   0.638991   \n",
              "3   context_precision        0.769006   0.750441   0.683211   \n",
              "4  answer_correctness        0.431944   0.695150   0.716742   \n",
              "\n",
              "   ADA + MQR -> TE3 + MQR  Baseline -> TE3 + MQR  \n",
              "0               -0.031976              -0.110182  \n",
              "1                0.076725              -0.013852  \n",
              "2               -0.136574               0.121447  \n",
              "3               -0.067230              -0.085794  \n",
              "4                0.021593               0.284798  "
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Ž# creating dataframe of all our retrivers\n",
        "\n",
        "df_baseline = pd.DataFrame(list(results.items()), columns=['Metric', 'ADA + Baseline'])\n",
        "df_original = pd.DataFrame(list(advanced_retrieval_results.items()), columns=['Metric', 'ADA + MQR'])\n",
        "df_comparison = pd.DataFrame(list(new_advanced_retrieval_results.items()), columns=['Metric', 'TE3 + MQR'])\n",
        "\n",
        "df_merged = pd.merge(df_original, df_comparison, on='Metric')\n",
        "df_merged = pd.merge(df_baseline, df_merged, on=\"Metric\")\n",
        "\n",
        "df_merged['ADA + MQR -> TE3 + MQR'] = df_merged['TE3 + MQR'] - df_merged['ADA + MQR']\n",
        "df_merged['Baseline -> TE3 + MQR'] = df_merged['TE3 + MQR'] - df_merged['ADA + Baseline']\n",
        "\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRmkcMrxC4Me"
      },
      "source": [
        "####❓ Question #4:\n",
        "\n",
        "Do you think, in your opinion, `text-embedding-3-small` is significantly better than `ada`?\n",
        "\n",
        "because i was getting token rate limit a lot during evaluation, i woukldn't say that my results are representative.\n",
        "\n",
        "Wiz reulsts were showing significant improvment using TE3 - text emb small regarding faithfullness, in my example te3 was better in answer relewancy and answer correctness.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOciJLABDBnA"
      },
      "source": [
        "## BONUS ACTIVITY: Using a Better Generator\n",
        "\n",
        "Now that we've seen how much more effective a better Retrieval pipeline is, let's look at what impact a better(?) Generator is!\n",
        "\n",
        "Adapt the above `TE3 + MQR` pipeline to use `GPT-4o` and compare the results below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY8l2EksDH43"
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.9 ('llmops-w5')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "c9e5523a24c5cd4eda77ee493767e4015a2e2bbdd0ec52ee3dcc95a5bf09d0e7"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "05390d20f1b445b5b02529ee7a99f6d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_589c2004f5504a239615dec8671785d0",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_25d3337c457f4c748ed8bf78f5a27fe8",
            "value": 100
          }
        },
        "05ab48866b5d49df9567ce9cbda5ee2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49c1ef316e404052a7c8528781db3f9a",
              "IPY_MODEL_2dcb3e2fdf164e35a27a79cfae65933a",
              "IPY_MODEL_202f4244384a4501bfc1ffa50af96a1f"
            ],
            "layout": "IPY_MODEL_d93698b0506743ff98fdb998cfb7080a"
          }
        },
        "1317f4e20e1c4574a360345b427c3e8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92ef10fab64c4f40a93da3d31b572016",
            "placeholder": "​",
            "style": "IPY_MODEL_3b43c3f561e34d019007ac9a0125b28d",
            "value": " 1248/1248 [00:46&lt;00:00, 13.97it/s]"
          }
        },
        "18701fc64eb44d26b8aa1ae0af64d09f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19acd28bfa2e4a7a83bc42faea5de770": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d43002974f24e8a8b6961cddc04ce47": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fb5a4b71deb406fa2f342c88b9e4e1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e1d22c19aff4c768d643c249e425d00",
            "placeholder": "​",
            "style": "IPY_MODEL_3a498872a68049329b4d206629b9b3bf",
            "value": "embedding nodes: 100%"
          }
        },
        "202f4244384a4501bfc1ffa50af96a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a10a7577a99b4683a1d59a09d88f93a1",
            "placeholder": "​",
            "style": "IPY_MODEL_444bc7dae1aa4e098b79655428599310",
            "value": " 20/20 [01:04&lt;00:00, 10.00s/it]"
          }
        },
        "25d3337c457f4c748ed8bf78f5a27fe8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2aa53858803d4ad39113009d86dd67fc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "2dcb3e2fdf164e35a27a79cfae65933a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60a663f8736a43bcb47ac6c5f37ec597",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e6edc46811064de2b74a6a477c4a44b7",
            "value": 20
          }
        },
        "31064d2adec14238a609d3f9791c64f3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32514310070a426ea247c9f1bc66b630": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3e7520df71de40e0af5589b6aeb95171",
              "IPY_MODEL_05390d20f1b445b5b02529ee7a99f6d6",
              "IPY_MODEL_3380693903474d2585638f7e3458fcd6"
            ],
            "layout": "IPY_MODEL_1d43002974f24e8a8b6961cddc04ce47"
          }
        },
        "3380693903474d2585638f7e3458fcd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31064d2adec14238a609d3f9791c64f3",
            "placeholder": "​",
            "style": "IPY_MODEL_4f482b8ce7a54c1787394fb7d90391a0",
            "value": " 100/100 [00:33&lt;00:00,  1.55s/it]"
          }
        },
        "356b929fa8dc42538767c58dcce12217": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37ec9b5c847749439d7c155ac3b1ec68": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89a7c333d0b241169dc29ed998b2c9c4",
            "max": 1248,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_88c8557741734e59a6099bb5fa260f6e",
            "value": 1248
          }
        },
        "38988d3f6f5f4de3b3d4be7fec89c3c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42dcc945d1624f69b63bed2fa52cc4fa",
            "placeholder": "​",
            "style": "IPY_MODEL_5524289f1e594a5eac60ee29d9f4249c",
            "value": " 100/100 [00:45&lt;00:00,  1.68s/it]"
          }
        },
        "399f6ec046c34c26818a07c5efc6845a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f82e0a3c3684460b8dda773d283b535",
              "IPY_MODEL_cfa01f60b62f4a88806d85cee5ac0fa6",
              "IPY_MODEL_38988d3f6f5f4de3b3d4be7fec89c3c7"
            ],
            "layout": "IPY_MODEL_87bd0ad74d4345dea4b409d64524f6e7"
          }
        },
        "3a498872a68049329b4d206629b9b3bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b43c3f561e34d019007ac9a0125b28d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e7520df71de40e0af5589b6aeb95171": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97abe811c89c44dcacd7e39074d22546",
            "placeholder": "​",
            "style": "IPY_MODEL_87a3d4b2ed5f4f1ca895c6a1981eb847",
            "value": "Evaluating: 100%"
          }
        },
        "40343486e3ea4e5fae55b5a528f139d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42dcc945d1624f69b63bed2fa52cc4fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "444bc7dae1aa4e098b79655428599310": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "49c1ef316e404052a7c8528781db3f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19acd28bfa2e4a7a83bc42faea5de770",
            "placeholder": "​",
            "style": "IPY_MODEL_356b929fa8dc42538767c58dcce12217",
            "value": "Generating: 100%"
          }
        },
        "4cefefc6cf714a68924e1b8d5e59aba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c2b92989d7448e9bf65306c4f2f7d93",
            "placeholder": "​",
            "style": "IPY_MODEL_a34b3906cd514234a115c7bf6757ca9d",
            "value": "Evaluating: 100%"
          }
        },
        "4f482b8ce7a54c1787394fb7d90391a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5524289f1e594a5eac60ee29d9f4249c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "589c2004f5504a239615dec8671785d0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c2b92989d7448e9bf65306c4f2f7d93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a663f8736a43bcb47ac6c5f37ec597": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e1d22c19aff4c768d643c249e425d00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f82e0a3c3684460b8dda773d283b535": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cb506949697432db061878397d196f1",
            "placeholder": "​",
            "style": "IPY_MODEL_e7831a581d024e3ebb4026a89ceef127",
            "value": "Evaluating: 100%"
          }
        },
        "831b4dab6ff94d239d2824d390e01308": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4cefefc6cf714a68924e1b8d5e59aba9",
              "IPY_MODEL_fd7f5542a22d44388dda12ca19443a1f",
              "IPY_MODEL_93bf9b194c04460abffa192c19bcf67b"
            ],
            "layout": "IPY_MODEL_83985f58744a46cfbd001ce5957f3e4a"
          }
        },
        "83985f58744a46cfbd001ce5957f3e4a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87a3d4b2ed5f4f1ca895c6a1981eb847": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "87bd0ad74d4345dea4b409d64524f6e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88c8557741734e59a6099bb5fa260f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "89a7c333d0b241169dc29ed998b2c9c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cb506949697432db061878397d196f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92ef10fab64c4f40a93da3d31b572016": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93bf9b194c04460abffa192c19bcf67b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cee2268aa21643e6ad77117c67ec1600",
            "placeholder": "​",
            "style": "IPY_MODEL_c79f28da88f44d69aa87905c089df333",
            "value": " 100/100 [00:44&lt;00:00,  1.69s/it]"
          }
        },
        "97abe811c89c44dcacd7e39074d22546": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a03fefb9fa5a40ff947dc4ccd3c80318": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a10a7577a99b4683a1d59a09d88f93a1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a34b3906cd514234a115c7bf6757ca9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6581091161c489d877c2cfec432f6ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c79f28da88f44d69aa87905c089df333": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cee2268aa21643e6ad77117c67ec1600": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfa01f60b62f4a88806d85cee5ac0fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18701fc64eb44d26b8aa1ae0af64d09f",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6581091161c489d877c2cfec432f6ae",
            "value": 100
          }
        },
        "d93698b0506743ff98fdb998cfb7080a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6edc46811064de2b74a6a477c4a44b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e7831a581d024e3ebb4026a89ceef127": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f75fdd56268a4b83a7fb7e4a3b2cce82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fb5a4b71deb406fa2f342c88b9e4e1d",
              "IPY_MODEL_37ec9b5c847749439d7c155ac3b1ec68",
              "IPY_MODEL_1317f4e20e1c4574a360345b427c3e8a"
            ],
            "layout": "IPY_MODEL_2aa53858803d4ad39113009d86dd67fc"
          }
        },
        "fd7f5542a22d44388dda12ca19443a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a03fefb9fa5a40ff947dc4ccd3c80318",
            "max": 100,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40343486e3ea4e5fae55b5a528f139d8",
            "value": 100
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
