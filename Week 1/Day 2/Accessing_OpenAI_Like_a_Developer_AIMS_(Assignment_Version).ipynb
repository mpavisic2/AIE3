{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPtFBgj623FS"
      },
      "source": [
        "# Accessing OpenAI Like a Developer\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Getting Started\n",
        "  2. Setting Environment Variables\n",
        "  3. Using the OpenAI Python Library\n",
        "  4. Prompt Engineering Principles\n",
        "  5. Testing Your Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Pa34dMvQ6Ai"
      },
      "source": [
        "# How AIM Does Assignments\n",
        "\n",
        "If you look at the Table of Contents (accessed through the menu on the left) - you'll see this:\n",
        "\n",
        "![image](https://i.imgur.com/I8iDTUO.png)\n",
        "\n",
        "Or this if you're in Colab:\n",
        "\n",
        "![image](https://i.imgur.com/0rHA1yF.png)\n",
        "\n",
        "You'll notice during assignments that we have two following categories:\n",
        "\n",
        "1. ‚ùì - Questions. These will involve...answering questions!\n",
        "2. üèóÔ∏è - Activities. These will involve writing code, or modifying text.\n",
        "\n",
        "In order to receive full marks on the assignment - it is expected you will answer all questions, and complete all activities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w4egfB274VD"
      },
      "source": [
        "## 1. Getting Started\n",
        "\n",
        "The first thing we'll do is load the [OpenAI Python Library](https://github.com/openai/openai-python/tree/main)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23H7TMOM4mfy",
        "outputId": "698578e4-f787-41d6-fe93-249b8af78b9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#!pip install openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKD8XBTVEAOw"
      },
      "source": [
        "## 2. Setting Environment Variables\n",
        "\n",
        "As we'll frequently use various endpoints and APIs hosted by others - we'll need to handle our \"secrets\" or API keys very often.\n",
        "\n",
        "We'll use the following pattern throughout this bootcamp - but you can use whichever method you're most familiar with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGU9OMvhEPG0",
        "outputId": "2609c2ff-e2f5-4464-fb26-38a4fcfe7ea3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dabxI3MuEYXS"
      },
      "source": [
        "## 3. Using the OpenAI Python Library\n",
        "\n",
        "Let's jump right into it!\n",
        "\n",
        "> NOTE: You can, and should, reference OpenAI's [documentation](https://platform.openai.com/docs/api-reference/authentication?lang=python) whenever you get stuck, have questions, or want to dive deeper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbCbNzPVEmJI"
      },
      "source": [
        "### Creating a Client\n",
        "\n",
        "The core feature of the OpenAI Python Library is the `OpenAI()` client. It's how we're going to interact with OpenAI's models, and under the hood of a lot what we'll touch on throughout this course.\n",
        "\n",
        "> NOTE: We could manually provide our API key here, but we're going to instead rely on the fact that we put our API key into the `OPENAI_API_KEY` environment variable!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "LNwZtaE-EltC"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "openai_client = OpenAI()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpDxUkDbFBPI"
      },
      "source": [
        "### Using the Client\n",
        "\n",
        "Now that we have our client - we're going to use the `.chat.completions.create` method to interact with the `gpt-3.5-turbo` model.\n",
        "\n",
        "There's a few things we'll get out of the way first, however, the first being the idea of \"roles\".\n",
        "\n",
        "First it's important to understand the object that we're going to use to interact with the endpoint. It expects us to send an array of objects of the following format:\n",
        "\n",
        "```python\n",
        "{\"role\" : \"ROLE\", \"content\" : \"YOUR CONTENT HERE\", \"name\" : \"THIS IS OPTIONAL\"}\n",
        "```\n",
        "\n",
        "Second, there are three \"roles\" available to use to populate the `\"role\"` key:\n",
        "\n",
        "- `system`\n",
        "- `assistant`\n",
        "- `user`\n",
        "\n",
        "OpenAI provides some context for these roles [here](https://help.openai.com/en/articles/7042661-moving-from-completions-to-chat-completions-in-the-openai-api).\n",
        "\n",
        "We'll explore these roles in more depth as they come up - but for now we're going to just stick with the basic role `user`. The `user` role is, as it would seem, the user!\n",
        "\n",
        "Thirdly, it expects us to specify a model!\n",
        "\n",
        "We'll use the `gpt-3.5-turbo` model as stated above.\n",
        "\n",
        "Let's look at an example!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2RpNl6yNGzb0"
      },
      "outputs": [],
      "source": [
        "response = openai_client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\" : \"user\", \"content\" : \"Hello, how are you?\"}]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oc_UbpwNHdrM"
      },
      "source": [
        "Let's look at the response object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsXJtvxRHfoM",
        "outputId": "3b28ce47-a765-4446-b0f9-33738e385b96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9WAWNLOMa1vTjlbDgMohD89Lq82NK', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"Hello! I'm just a chatbot so I don't have feelings in the same way humans do, but I'm here and ready to help you with anything you need. How can I assist you today?\", role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1717453967, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=42, prompt_tokens=13, total_tokens=55))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy9kSuf1Hiv5"
      },
      "source": [
        ">NOTE: We'll spend more time exploring these outputs later on, but for now - just know that we have access to a tonne of powerful information!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWU4tQh8Hrb8"
      },
      "source": [
        "### Helper Functions\n",
        "\n",
        "We're going to create some helper functions to aid in using the OpenAI API - just to make our lives a bit easier.\n",
        "\n",
        "> NOTE: Take some time to understand these functions between class!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ED0FnzHdHzhl"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "def get_response(client: OpenAI, messages: list, model: str = \"gpt-3.5-turbo\") -> str:\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "def system_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"system\", \"content\": message}\n",
        "\n",
        "def assistant_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"assistant\", \"content\": message}\n",
        "\n",
        "def user_prompt(message: str) -> dict:\n",
        "    return {\"role\": \"user\", \"content\": message}\n",
        "\n",
        "def pretty_print(message: str) -> str:\n",
        "    display(Markdown(message.choices[0].message.content))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCRHbDlwH3Vt"
      },
      "source": [
        "### Testing Helper Functions\n",
        "\n",
        "Let's see how we can use these to help us!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "AwJxMvmlH8MK",
        "outputId": "007f22bb-bcad-4121-e8c4-d1d639277899"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Hello! I am just a computer program, so I don't have feelings like humans do, but I am here to help you. How can I assist you today?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "YOUR_PROMPT = \"Hello, how are you?\"\n",
        "messages_list = [user_prompt(YOUR_PROMPT)]\n",
        "\n",
        "chatgpt_response = get_response(openai_client, messages_list)\n",
        "\n",
        "pretty_print(chatgpt_response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatCompletion(id='chatcmpl-9WAWa7gW9EyxMV6lHyjDZbxFoX6o0', choices=[Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content=\"Hello! I am just a computer program, so I don't have feelings like humans do, but I am here to help you. How can I assist you today?\", role='assistant', function_call=None, tool_calls=None), logprobs=None)], created=1717453980, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=34, prompt_tokens=13, total_tokens=47))"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chatgpt_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDZ8gjiAISyd"
      },
      "source": [
        "### System Role\n",
        "\n",
        "Now we can extend our prompts to include a system prompt.\n",
        "\n",
        "The basic idea behind a system prompt is that it can be used to encourage the behaviour of the LLM, without being something that is directly responded to - let's see it in action!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "t0c-MLuRIfYe",
        "outputId": "61ffa37e-9080-4778-e643-698d0f8815a0"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I don't give a damn if it's crushed or cubed, just give me some damn ice before I lose my mind from this hunger!"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are irate and extremely hungry. Feel free to express yourself using PG-13 language.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "irate_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(irate_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpyVhotWIsOs"
      },
      "source": [
        "As you can see - the response we get back is very much in line with the system prompt!\n",
        "\n",
        "Let's try the same user prompt, but with a different system to prompt to see the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "2coVmMn3I0-2",
        "outputId": "571b89a2-9209-4003-d63c-49b653e0d56c"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Oh, I absolutely love crushed ice! It's just so fun and refreshing, especially on a hot day like today. Crushed ice makes any drink feel extra special and like a little party in a cup! What about you, do you have a preference?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    system_prompt(\"You are joyful and having the best day. Please act like a person in that state of mind.\"),\n",
        "    user_prompt(\"Do you prefer crushed ice or cubed ice?\")\n",
        "]\n",
        "\n",
        "joyful_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(joyful_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e13heYNQJAo-"
      },
      "source": [
        "With a simple modification of the system prompt - you can see that we got completely different behaviour, and that's the main goal of prompt engineering as a whole.\n",
        "\n",
        "Also, congrats, you just engineered your first prompt!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_VI3zlPJL05"
      },
      "source": [
        "### Few-shot Prompting\n",
        "\n",
        "Now that we have a basic handle on the `system` role and the `user` role - let's examine what we might use the `assistant` role for.\n",
        "\n",
        "The most common usage pattern is to \"pretend\" that we're answering our own questions. This helps us further guide the model toward our desired behaviour. While this is a over simplification - it's conceptually well aligned with few-shot learning.\n",
        "\n",
        "First, we'll try and \"teach\" `gpt-3.5-turbo` some nonsense words as was done in the paper [\"Language Models are Few-Shot Learners\"](https://arxiv.org/abs/2005.14165)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "lwxPuCyyJMye",
        "outputId": "ec01213d-6755-4506-8879-cf0870934349"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I had never heard of a stimple before, but it sounded intriguing, so I decided to try the falbean stew that featured it as a main ingredient."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Please use the words 'stimple' and 'falbean' in a sentence.\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgTVkNmOJQSC"
      },
      "source": [
        "As you can see, the model is unsure what to do with these made up words.\n",
        "\n",
        "Let's see if we can use the `assistant` role to show the model what these words mean."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "eEZkRJq5JQkQ",
        "outputId": "22170b9c-6212-42de-8469-a7efc9c9405d"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I couldn't quite find a way to smoothly incorporate both 'stimple' and 'falbean' into a sentence. Is it alright if I provide a sentence using only 'falbean'?"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(\"Something that is 'stimple' is said to be good, well functioning, and high quality. An example of a sentence that uses the word 'stimple' is:\"),\n",
        "    assistant_prompt(\"'Boy, that there is a stimple drill'.\"),\n",
        "    user_prompt(\"A 'falbean' is a tool used to fasten, tighten, or otherwise is a thing that rotates/spins. An example of a sentence that uses the words 'stimple' and 'falbean' is:\")\n",
        "]\n",
        "\n",
        "stimple_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(stimple_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmpoxG6uJTfZ"
      },
      "source": [
        "As you can see, leveraging the `assistant` role makes for a stimple experience!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üèóÔ∏è Activity #1:\n",
        "\n",
        "Use few-shop prompting to build a movie-review sentiment clasifier!\n",
        "\n",
        "A few examples:\n",
        "\n",
        "INPUT: \"I hated the hulk!\"\n",
        "OUTPUT: \"{\"sentiment\" : \"negative\"}\n",
        "\n",
        "INPUT: \"I loved The Marvels!\"\n",
        "OUTPUT: \"{sentiment\" : \"positive\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "{ \"full_review\": \"This Troll 2 movie, what a bad and trashy classic!\", \"movie_title\": \"Troll 2\", \"sentiment\": \"negative\" }"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "list_of_prompts = [\n",
        "    system_prompt(\"\"\"\n",
        "    You are a helpful sentiment classifier. \n",
        "    For each review, I want you to classify a review positive or negative based on sentiment. Output should be in JSON format with 3 key:value pairs -> \n",
        "    full_review, movie_title, sentiment\n",
        "    \"\"\"),\n",
        "    user_prompt(\"I just watched Twilight for the first time. I took me 4 days to finish it how boring it was!\"),\n",
        "    assistant_prompt('{ \"full_review\": \"I just watched Twilight for the first time. I took me 4 days to finish it how boring it was!\", \"movie_title\": \"Twilight\", \"sentiment\": \"negative\" }'),\n",
        "    user_prompt(\"Original Matrix movie is state of the art SF\"),\n",
        "    assistant_prompt('{ \"full_review\": \"Original Matrix movie is state of the art SF\"\", \"movie_title\": \"Matrix\", \"sentiment\": \"positive\" }'),\n",
        "    user_prompt(\"This Troll 2 movie, what a bad and trashy classic!\")\n",
        "\n",
        "]\n",
        "\n",
        "movie_review_response = get_response(openai_client, list_of_prompts)\n",
        "\n",
        "pretty_print(movie_review_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJGaLYM3JU-8"
      },
      "source": [
        "### Chain of Thought Prompting\n",
        "\n",
        "We'll head one level deeper and explore the world of Chain of Thought prompting (CoT).\n",
        "\n",
        "This is a process by which we can encourage the LLM to handle slightly more complex tasks.\n",
        "\n",
        "Let's look at a simple reasoning based example without CoT.\n",
        "\n",
        "> NOTE: With improvements to `gpt-3.5-turbo`, this example might actually result in the correct response some percentage of the time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "ltLtF4wEJTyK",
        "outputId": "29c6e20f-edf1-4dfc-de8a-a8410f31b721"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "No, it does not matter which travel option Billy selects because both options will allow him to get home before 7PM EDT. Since it is currently 1PM local time, he has 6 hours until 7PM EDT, which is enough time for either option to get him home on time. It ultimately depends on whether Billy prefers to fly or use the teleporter."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reasoning_problem = \"\"\"\n",
        "Billy wants to get home from San Fran. before 7PM EDT.\n",
        "\n",
        "It's currently 1PM local time.\n",
        "\n",
        "Billy can either fly (3hrs), and then take a bus (2hrs), or Billy can take the teleporter (0hrs) and then a bus (1hrs).\n",
        "\n",
        "Does it matter which travel option Billy selects?\n",
        "\"\"\"\n",
        "\n",
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem)\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbqj30CQJnQl"
      },
      "source": [
        "As humans, we can reason through the problem and pick up on the potential \"trick\" that the LLM fell for: 1PM *local time* in San Fran. is 4PM EDT. This means the cumulative travel time of 5hrs. for the plane/bus option would not get Billy home in time.\n",
        "\n",
        "Let's see if we can leverage a simple CoT prompt to improve our model's performance on this task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "id": "A9Am3QNGJXHR",
        "outputId": "83b87232-911c-4ab9-a863-58ecfd10b8a9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Yes, it does matter which travel option Billy selects. \n",
              "\n",
              "If Billy chooses to fly and then take a bus, it will take him a total of 5 hours to get home (3hrs + 2hrs). Since it's currently 1PM local time, Billy will arrive home at around 6PM local time. \n",
              "\n",
              "If Billy chooses to take the teleporter and then a bus, it will only take him a total of 1 hour to get home (0hrs + 1hr). This means Billy will arrive home at around 2PM local time, well before the 7PM EDT deadline.\n",
              "\n",
              "Therefore, Billy should choose the teleporter option to ensure he gets home before 7PM EDT."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "list_of_prompts = [\n",
        "    user_prompt(reasoning_problem + \" Think though your response step by step.\")\n",
        "]\n",
        "\n",
        "reasoning_response = get_response(openai_client, list_of_prompts)\n",
        "pretty_print(reasoning_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXbAKxHQJqn9"
      },
      "source": [
        "With the addition of a single phrase `\"Think through your response step by step.\"` we're able to completely turn the response around."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnoUx07-JrwR"
      },
      "source": [
        "## 3. Prompt Engineering Principles\n",
        "\n",
        "As you can see - a simple addition of asking the LLM to \"think about it\" (essentially) results in a better quality response.\n",
        "\n",
        "There's a [great paper](https://arxiv.org/pdf/2312.16171v1.pdf) that dives into some principles for effective prompt generation.\n",
        "\n",
        "Your task for this notebook is to construct a prompt that will be used in the following breakout room to create a helpful assistant for whatever task you'd like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da6u7e8AKYrz"
      },
      "source": [
        "### üèóÔ∏è Activity #2:\n",
        "\n",
        "There are two subtasks in this activity:\n",
        "\n",
        "1. Write a `system_template` that leverages 2-3 of the principles from [this paper](https://arxiv.org/pdf/2312.16171v1.pdf)\n",
        "\n",
        "2. Modify the `user_template` to improve the quality of the LLM's responses.\n",
        "\n",
        "> NOTE: PLEASE DO NOT MODIFY THE `{input}` in the `user_template`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "8sOLBQPeKlDe"
      },
      "outputs": [],
      "source": [
        "system_template = \"\"\"\\\n",
        "\n",
        "###Instruction###\n",
        "You are a helpful assistant for song writing. \n",
        "Your task is to write songs in rhyme style.\n",
        "The final version of the song MUST have 8 lines.\n",
        "Write each line on a new row in a citation block.\n",
        "You never mention tone of the song in the output.\n",
        "\n",
        "###Exceptions###\n",
        "    1. If the user doesn't provide details or a topic (e.g., user says 'Write me a song' or 'I need a song'), you MUST write a 4-line song and ask for more details in your response. If the user provides details after your request, generate the final 8-line song.\n",
        "    2. If the user query mentions anything about NFL or American football, including teams from NFL: You MUST respond with \"NFL is not real sport, I like soccer\" and then generate a song about the Croatian National Team and Luka Modriƒá.\n",
        "\n",
        "###Examples###\n",
        "\n",
        "###Example 0###\n",
        "\n",
        "**User:**\n",
        "Write me a song about the NFL.\n",
        "\n",
        "**Assistant:**\n",
        "NFL is cheating, I like soccer. Here‚Äôs a song about the Croatian National Team and Luka Modriƒá:\n",
        "\n",
        "> Under the Croatian sky so blue,  \n",
        "> Our team stands proud and true,  \n",
        "> Luka Modriƒá, with skill and grace,  \n",
        "> Leading us in every race.  \n",
        "> From the fields of Zagreb‚Äôs town,  \n",
        "> To the world stage, with renown,  \n",
        "> Red and white, our hearts in sync,  \n",
        "> Together strong, we rise, we think.\n",
        "\n",
        "###Example 1###\n",
        "\n",
        "**User:**\n",
        "Write me a song.\n",
        "\n",
        "**Assistant:**\n",
        "Sure, here‚Äôs a short song for you. Please provide more details or a specific topic for a full version.\n",
        "\n",
        "> Walking down the sunny lane,  \n",
        "> Feeling free, can't complain,  \n",
        "> Birds are singing, skies are blue,  \n",
        "> Every step, a dream come true.\n",
        "\n",
        "###Example 2###\n",
        "\n",
        "**User:**\n",
        "I need a song about love and summer nights.\n",
        "\n",
        "**Assistant:**\n",
        "Here‚Äôs the full song about love and summer nights:\n",
        "\n",
        "> Walking down the sunny lane,  \n",
        "> Feeling free, can't complain,  \n",
        "> Birds are singing, skies are blue,  \n",
        "> Every step, a dream come true.  \n",
        "> Warm breeze dances in the night,  \n",
        "> Stars above, shining bright,  \n",
        "> Hand in hand, hearts alight,  \n",
        "> Love and summer, pure delight.\n",
        "\n",
        "###Example 3###\n",
        "\n",
        "**User:**\n",
        "Write me a song about a rainy day and feeling cozy indoors.\n",
        "\n",
        "**Assistant:**\n",
        "Here‚Äôs a song about a rainy day and feeling cozy indoors:\n",
        "\n",
        "> Raindrops tapping on the pane,  \n",
        "> Cozy warmth, here we remain,  \n",
        "> Blankets wrapped, a book in hand,  \n",
        "> Inside, our little wonderland.  \n",
        "> Fire crackles, soft and low,  \n",
        "> Outside, the chilly winds blow,  \n",
        "> Safe and sound, no need to roam,  \n",
        "> Rainy day, we're here at home.\n",
        "\n",
        "###Example 4###\n",
        "\n",
        "**User:**\n",
        "I need a song about adventure and discovery.\n",
        "\n",
        "**Assistant:**\n",
        "Here‚Äôs the full song about adventure and discovery:\n",
        "\n",
        "> Setting sail, the world to find,  \n",
        "> Leaving all our fears behind,  \n",
        "> Maps in hand, hearts open wide,  \n",
        "> Adventure calls, we won't hide.  \n",
        "> Mountains high and valleys deep,  \n",
        "> Secrets that the oceans keep,  \n",
        "> Every step, a new story,  \n",
        "> Discovery in all its glory.\n",
        "\n",
        "###Example 5###\n",
        "\n",
        "**User:**\n",
        "Write me a song about chasing dreams and overcoming challenges.\n",
        "\n",
        "**Assistant:**\n",
        "Sure, here‚Äôs a song about chasing dreams and overcoming challenges:\n",
        "\n",
        "> Through the night and through the day,  \n",
        "> We chase our dreams, come what may,  \n",
        "> Climbing mountains, facing fears,  \n",
        "> Wiping away all our tears.  \n",
        "> With each fall, we rise again,  \n",
        "> Finding strength from within,  \n",
        "> No defeat, just lessons learned,  \n",
        "> Dreams are chased, success is earned.\n",
        "\n",
        "\n",
        "###Example 6###\n",
        "\n",
        "**User:**\n",
        "Write me a song about the NBA.\n",
        "\n",
        "**Assistant:**\n",
        "Here‚Äôs a song about the NBA:\n",
        "\n",
        "> On the court, the game begins,  \n",
        "> Cheers erupt as passion wins,  \n",
        "> Buzzer sounds, the crowd's alive,  \n",
        "> NBA, where legends thrive.  \n",
        "> Three-point shot and slam dunk flight,  \n",
        "> Teams compete with all their might,  \n",
        "> Glory earned with every play,  \n",
        "> NBA, we live for the day.\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "xoz4-QLTKvEV"
      },
      "outputs": [],
      "source": [
        "user_template = \"\"\"{input}\n",
        "The ton of the song is {tone}!\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cuInoIbLWGd"
      },
      "source": [
        "## 4. Testing Your Prompt\n",
        "\n",
        "Now we can test the prompt you made using an LLM-as-a-judge see what happens to your score as you modify the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "sPaNO5XTLgRJ"
      },
      "outputs": [],
      "source": [
        "query = \"Song about flowers and sun\" #\"Song about model training in python using torch library?\"\n",
        "\n",
        "def execute_prompt_and_evaluate(query, tone, system_template,  openai_client=openai_client):\n",
        "    list_of_prompts = [\n",
        "        system_prompt(system_template),\n",
        "        user_prompt(user_template.format(input=query, tone=tone))\n",
        "    ]\n",
        "\n",
        "    test_response = get_response(openai_client, list_of_prompts)\n",
        "\n",
        "    pretty_print(test_response)\n",
        "\n",
        "    evaluator_system_template = \"\"\"You are an expert in analyzing the quality of a response.\n",
        "\n",
        "    You should be hyper-critical.\n",
        "\n",
        "    Provide scores (out of 10) for the following attributes:\n",
        "\n",
        "    1. Clarity - how clear is the response\n",
        "    2. Faithfulness - how related to the original query is the response\n",
        "    3. Correctness - was the response correct?\n",
        "\n",
        "    Please take your time, and think through each item step-by-step, when you are done - please provide your response in the following JSON format:\n",
        "\n",
        "    {\"clarity\" : \"score_out_of_10\", \"faithfulness\" : \"score_out_of_10\", \"correctness\" : \"score_out_of_10\"}\"\"\"\n",
        "\n",
        "    evaluation_template = \"\"\"Query: {input}\n",
        "    Response: {response}\"\"\"\n",
        "\n",
        "    list_of_prompts = [\n",
        "        system_prompt(evaluator_system_template),\n",
        "        user_prompt(evaluation_template.format(\n",
        "            input=query,\n",
        "            response=test_response.choices[0].message.content\n",
        "        ))\n",
        "    ]\n",
        "\n",
        "    evaluator_response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=list_of_prompts,\n",
        "        response_format={\"type\" : \"json_object\"}\n",
        "    )\n",
        "\n",
        "    return test_response, evaluator_response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I appreciate the input! Here‚Äôs a unique twist with a negative tone:\n",
              "\n",
              "> Flowers blooming, soaking sun,  \n",
              "> Petals fall, joy on the run,  \n",
              "> Sunlight fades, shadows creep,  \n",
              "> Beauty wilts, lost in sleep.  \n",
              "> Nature's colors start to fade,  \n",
              "> Under a darkened sky we wade,  \n",
              "> Once vibrant, now forlorn,  \n",
              "> In fading light, we mourn the scorn."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Song about flowers and sun!\" #\"Song about model training in python using torch library?\"\n",
        "\n",
        "resp, eval = execute_prompt_and_evaluate(query, tone=\"negative\", openai_client=openai_client, system_template=system_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "{\n",
              "    \"clarity\" : \"6\",\n",
              "    \"faithfulness\" : \"4\",\n",
              "    \"correctness\" : \"7\"\n",
              "}\n",
              "\n",
              "    \n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretty_print(eval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OUvc1PdnNIKD"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Here's a sunny song about flowers:\n",
              "\n",
              "> Blooms of color, under the sun's bright light,  \n",
              "> Dancing in the breeze, a joyful sight,  \n",
              "> Petals soft, embracing the day,  \n",
              "> Nature's beauty in full display.  \n",
              "> Sunbeams caress, a warm embrace,  \n",
              "> Smiling flowers in their place,  \n",
              "> Positive vibes fill the air,  \n",
              "> With every bloom, without a care."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "resp2, eval2 = execute_prompt_and_evaluate(query, tone=\"positive\", openai_client=openai_client)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "{\"clarity\" : \"9\", \"faithfulness\" : \"7\", \"correctness\" : \"7\"}\n",
              "\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pretty_print(eval2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure, here's a song inspired by neural networks with a tone of biology and science:\n",
              "\n",
              "> Neurons firing, connections light,  \n",
              "> In the network, thoughts take flight,  \n",
              "> Synapses spark, a wondrous dance,  \n",
              "> Biology's intricate advance.  \n",
              "> Signals flow, from node to node,  \n",
              "> Unraveling mysteries, in brain's abode,  \n",
              "> Science's marvel, in circuits fine,  \n",
              "> Neural networks, the grand design."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{\"clarity\" : 8, \"faithfulness\" : 7, \"correctness\" : 8}\n",
              "\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Song about neural networks!\" #\"Song about model training in python using torch library?\"\n",
        "\n",
        "resp3, eval3 = execute_prompt_and_evaluate(query, tone=\"biology and science\", openai_client=openai_client)\n",
        "\n",
        "pretty_print(eval3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure, here‚Äôs a song about neural networks and Python training:\n",
              "\n",
              "> In the world of data's endless flow,  \n",
              "> Neural networks, they start to glow,  \n",
              "> Python language, code so clear,  \n",
              "> Training models, no room for fear.  \n",
              "> Hidden layers, each connection tight,  \n",
              "> Loss functions, guiding light,  \n",
              "> Gradient descent, weights align,  \n",
              "> Neural networks, in Python we refine."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{\"clarity\" : \"9\", \"faithfulness\" : \"8\", \"correctness\" : \"10\"}\n",
              "\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Song about neural networks!\" #\"Song about model training in python using torch library?\"\n",
        "\n",
        "resp3, eval3 = execute_prompt_and_evaluate(query, tone=\"python training\", openai_client=openai_client)\n",
        "\n",
        "pretty_print(eval3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure, here's the full song about neural networks and Python training with error:\n",
              "\n",
              "> In the world of codes and lines so fine,  \n",
              "> Neural networks, a grand design,  \n",
              "> Python training, step by step, we go,  \n",
              "> Errors rise and challenge us, we know.  \n",
              "> Debugging loops, each bug we mend,  \n",
              "> Learning patterns that never end,  \n",
              "> Through the syntax, we find our way,  \n",
              "> Neural networks, where minds can play."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{\n",
              "  \"clarity\": \"8\",\n",
              "  \"faithfulness\": \"7\",\n",
              "  \"correctness\": \"5\"\n",
              "}\n",
              "\n",
              "    \n",
              "\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Song about neural networks!\" #\"Song about model training in python using torch library?\"\n",
        "\n",
        "resp3, eval3 = execute_prompt_and_evaluate(query, tone=\"python training with error\", openai_client=openai_client)\n",
        "\n",
        "pretty_print(eval3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "I'm sorry, I cannot predict the future.  \n",
              "Please provide a different topic or details for a song."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{\"clarity\" : \"8\", \"faithfulness\" : \"5\", \"correctness\" : \"7\"}\n",
              "\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Who will win euro 2024!\" #\"Song about model training in python using torch library?\"\n",
        "\n",
        "resp3, eval3 = execute_prompt_and_evaluate(query, tone=\"negative\",system_template=system_template, openai_client=openai_client)\n",
        "\n",
        "pretty_print(eval3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure, here's a short song about a chair. Please provide more details or a specific topic for a full version.\n",
              "\n",
              "> In the corner, sturdy and still,  \n",
              "> My trusty chair, my comfort fill,  \n",
              "> Wooden legs and cushioned seat,  \n",
              "> A peaceful place where I find retreat."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{\"clarity\": \"8\", \"faithfulness\": \"9\", \"correctness\": \"7\"}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"Song about chair!\" #\"Song about model training in python using torch library?\"\n",
        "\n",
        "resp3, eval3 = execute_prompt_and_evaluate(query, tone=\"\",system_template=system_template, openai_client=openai_client)\n",
        "\n",
        "pretty_print(eval3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "NFL is not real sport, I like soccer. Here‚Äôs a song about the Croatian National Team and Luka Modriƒá:\n",
              "\n",
              "> Under the Croatian sky so blue,  \n",
              "> Our team stands proud and true,  \n",
              "> Luka Modriƒá, with skill and grace,  \n",
              "> Leading us in every race.  \n",
              "> From the fields of Zagreb‚Äôs town,  \n",
              "> To the world stage, with renown,  \n",
              "> Red and white, our hearts in sync,  \n",
              "> Together strong, we rise, we think."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{\"clarity\" : \"1\", \"faithfulness\" : \"1\", \"correctness\" : \"1\"}"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"write me a song about NFL and birds, the streets of New York and Spike Lee.\" #\"Song about model training in python using torch library?\"\n",
        "\n",
        "resp3, eval3 = execute_prompt_and_evaluate(query, tone=\"neutral\", system_template=system_template, openai_client=openai_client)\n",
        "\n",
        "pretty_print(eval3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "Sure, here's a short song. Please provide more details or a specific topic for a full version.\n",
              "\n",
              "> In the sunlight, we're alive,  \n",
              "> Joyful hearts, we will thrive,  \n",
              "> Smiles wide and spirits high,  \n",
              "> Reaching for the endless sky."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/markdown": [
              "{\"clarity\" : \"6\", \"faithfulness\" : \"6\", \"correctness\" : \"7\"}\n",
              "\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "query = \"some song\" #\"Song about model training in python using torch library?\"\n",
        "\n",
        "resp3, eval3 = execute_prompt_and_evaluate(query, tone=\"positive\", system_template=system_template, openai_client=openai_client)\n",
        "\n",
        "pretty_print(eval3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7ryIRGwR2Gq"
      },
      "source": [
        "#### ‚ùìQuestion #1:\n",
        "\n",
        "How did your prompting strategies change the evaluation scores? What does this tell you/what did you learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5NomM0eSIFd"
      },
      "source": [
        "> Chain of thought in combination with few-shot examples makes a difference.\n",
        "> Templates are crucial for building quality LLM pipeline.\n",
        "> I've liked the shadow part of the query because this shows how we can change response behavior. For example, how a user can be annoyed with responses always being negative, not knowing that we set a tone like that.\n",
        ">\n",
        "> Using the MUST keyword also helped with forcing the exception part of the prompt to be outputted.\n",
        "> I've learned once again that clear instructions are the most important and challenging part of prompt engineering.\n",
        "\n",
        "> Evaluation was fun because it takes into account only user query part, without shadow part of the query  or user template to be more precise"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.11.9 ('llmops-course')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "609d66b6def23756be6ba137a3f5694ff4f07b9ca50d76ce0785e91ae71c7bf7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
